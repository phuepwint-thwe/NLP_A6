{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWNry4vhWuJN",
        "outputId": "89384c37-b600-402f-a4dc-3730e8860ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting huggingface_hub==0.16.4\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (3.17.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2025.1.31)\n",
            "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.28.1\n",
            "    Uninstalling huggingface-hub-0.28.1:\n",
            "      Successfully uninstalled huggingface-hub-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "transformers 4.48.3 requires huggingface-hub<1.0,>=0.24.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "accelerate 1.3.0 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.16.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.16.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsOtVCSMW5rv",
        "outputId": "1f88370f-e58d-4996-9f3d-26105e353112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: langchain 0.3.20\n",
            "Uninstalling langchain-0.3.20:\n",
            "  Successfully uninstalled langchain-0.3.20\n",
            "\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: langchain-core 0.3.44\n",
            "Uninstalling langchain-core-0.3.44:\n",
            "  Successfully uninstalled langchain-core-0.3.44\n",
            "\u001b[33mWARNING: Skipping faiss as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping faiss-cpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping faiss-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping InstructorEmbedding as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y langchain langchain-community langchain-core faiss faiss-cpu faiss-gpu InstructorEmbedding numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJFw1jY9XC2r",
        "outputId": "b6465912-0ec6-4bc7-f804-00c61959a43e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.1.10\n",
            "  Downloading langchain-0.1.10-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-community==0.0.27\n",
            "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (3.11.13)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.10)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (1.33)\n",
            "Collecting langchain-core<0.2,>=0.1.28 (from langchain==0.1.10)\n",
            "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.10)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain==0.1.10)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1 (from langchain==0.1.10)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.1.10) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.10)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.10) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.10)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.10)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.10) (3.0.0)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.28->langchain==0.1.10)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.10) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.1.10) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.1.10) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.10) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.10)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain==0.1.10) (1.3.1)\n",
            "Downloading langchain-0.1.10-py3-none-any.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, packaging, numpy, mypy-extensions, typing-inspect, marshmallow, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.13\n",
            "    Uninstalling langsmith-0.3.13:\n",
            "      Successfully uninstalled langsmith-0.3.13\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.6\n",
            "    Uninstalling langchain-text-splitters-0.3.6:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "transformers 4.48.3 requires huggingface-hub<1.0,>=0.24.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "accelerate 1.3.0 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.16.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-0.1.10 langchain-community-0.0.27 langchain-core-0.1.53 langchain-text-splitters-0.0.2 langsmith-0.1.147 marshmallow-3.26.1 mypy-extensions-1.0.0 numpy-1.26.4 packaging-23.2 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "Collecting accelerate==0.25.0\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting transformers==4.35.2\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.41.2\n",
            "  Downloading bitsandbytes-0.41.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (0.16.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.35.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.25.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.25.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.25.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.35.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (3.0.2)\n",
            "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.41.2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.3.0\n",
            "    Uninstalling accelerate-1.3.0:\n",
            "      Successfully uninstalled accelerate-1.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 bitsandbytes-0.41.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.15.2 transformers-4.35.2\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting InstructorEmbedding==1.0.1\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.14.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.17.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.1.31)\n",
            "Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=b2cdff5411e507e5f1d760b8028828792e84deb4fdc1b32c5d6fffccaf11c921\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: InstructorEmbedding, sentence-transformers\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.4.1\n",
            "    Uninstalling sentence-transformers-3.4.1:\n",
            "      Successfully uninstalled sentence-transformers-3.4.1\n",
            "Successfully installed InstructorEmbedding-1.0.1 sentence-transformers-2.2.2\n",
            "Collecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu==1.7.2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu==1.7.2\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pymupdf==1.23.8\n",
            "  Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.23.7 (from pymupdf==1.23.8)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.7 pymupdf-1.23.8\n"
          ]
        }
      ],
      "source": [
        "# LangChain\n",
        "!pip install langchain==0.1.10 langchain-community==0.0.27\n",
        "\n",
        "# LLM and Transformers\n",
        "!pip install accelerate==0.25.0 transformers==4.35.2 bitsandbytes==0.41.2\n",
        "\n",
        "# Text Embedding\n",
        "!pip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1\n",
        "\n",
        "# FAISS Vector Store (Install CPU version first, then upgrade to GPU)\n",
        "!pip install faiss-cpu==1.7.4\n",
        "!pip install faiss-gpu==1.7.2 --no-cache-dir\n",
        "\n",
        "# PDF Processing (if needed)\n",
        "!pip install pymupdf==1.23.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93iuj1-3YKUx",
        "outputId": "cf4a1668-bba4-494d-f15c-9b9c460f3ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2VJlATJYKSG",
        "outputId": "09ef83fb-9bbb-4fb0-c1b6-ac12714494e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: bitsandbytes 0.41.2\n",
            "Uninstalling bitsandbytes-0.41.2:\n",
            "  Successfully uninstalled bitsandbytes-0.41.2\n",
            "Collecting bitsandbytes==0.41.2\n",
            "  Downloading bitsandbytes-0.41.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Downloading bitsandbytes-0.41.2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m218.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes==0.41.2 --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9MSK_jkae4fj",
        "outputId": "79b593b6-72b5-45ac-f64e-81115c354345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: triton 3.2.0\n",
            "Uninstalling triton-3.2.0:\n",
            "  Successfully uninstalled triton-3.2.0\n",
            "Collecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.31.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (2.6.0+cu124)\n",
            "Collecting lit (from triton==2.0.0)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (12.4.127)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->triton==2.0.0) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting torch (from triton==2.0.0)\n",
            "  Downloading torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch->triton==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->triton==2.0.0) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->triton==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->triton==2.0.0) (1.3.0)\n",
            "Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m314.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m284.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m321.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m321.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m293.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m248.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m363.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m256.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m293.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m291.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m299.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m262.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m322.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m290.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, triton\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "713e805871444d49a0667de2bbf7b5f0",
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y triton\n",
        "!pip install triton==2.0.0 --no-cache-dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4d6bPNDeuTg",
        "outputId": "e3a63c08-bd3c-4b77-f377-9ed382dcd688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "++++++++++++++++++ /usr/local CUDA PATHS +++++++++++++++++++\n",
            "/usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/torch/lib/libc10_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda_linalg.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda120.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda114.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda122_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda111_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda120_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda114_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda115.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda111.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda110_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda110.so\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so\n",
            "/usr/local/lib/python3.11/dist-packages/libucx/lib/ucx/libucm_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/libucx/lib/ucx/libucx_perftest_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/libucx/lib/ucx/libuct_cuda.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/cudart.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/ccudart.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/ccuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuda/cuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/rmm/pylibrmm/cuda_stream.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/pylibraft/common/cuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.11/dist-packages/jax_cuda12_plugin/cuda_plugin_extension.so\n",
            "/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/xla_cuda_plugin.so\n",
            "/usr/local/lib/python3.11/dist-packages/cuml/common/cuda.cpython-311-x86_64-linux-gnu.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcudart.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/stubs/libcuda.so\n",
            "/usr/local/cuda-12.5/compat/libcuda.so\n",
            "\n",
            "+++++++++++++++ WORKING DIRECTORY CUDA PATHS +++++++++++++++\n",
            "\n",
            "\n",
            "++++++++++++++++++ LD_LIBRARY CUDA PATHS +++++++++++++++++++\n",
            "+++++++++++++++ /usr/lib64-nvidia CUDA PATHS +++++++++++++++\n",
            "/usr/lib64-nvidia/libcuda.so\n",
            "\n",
            "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
            "COMPILED_WITH_CUDA = True\n",
            "COMPUTE_CAPABILITIES_PER_GPU = ['7.5']\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Running a quick check that:\n",
            "    + library is importable\n",
            "    + CUDA function is callable\n",
            "\n",
            "\n",
            "WARNING: Please be sure to sanitize sensible info from any such env vars!\n",
            "\n",
            "SUCCESS!\n",
            "Installation was successful!\n"
          ]
        }
      ],
      "source": [
        "!python -m bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qNLAhPU1euQH"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0f3K_TKkpBK",
        "outputId": "7e693188-35d9-49c8-8abf-c1344518c505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.1.10)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.41 (from langchain)\n",
            "  Downloading langchain_core-0.3.45-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.45-py3-none-any.whl (415 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.9/415.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.53\n",
            "    Uninstalling langchain-core-0.1.53:\n",
            "      Successfully uninstalled langchain-core-0.1.53\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.0.2\n",
            "    Uninstalling langchain-text-splitters-0.0.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.0.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.1.10\n",
            "    Uninstalling langchain-0.1.10:\n",
            "      Successfully uninstalled langchain-0.1.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.0.27 requires langchain-core<0.2.0,>=0.1.30, but you have langchain-core 0.3.45 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.3.20 langchain-core-0.3.45 langchain-openai-0.3.8 langchain-text-splitters-0.3.6 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kag0vRQUko9u",
        "outputId": "74ed99d8-7bdf-4033-dac7-2d8921afadac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWZ6O_eRmF5m",
        "outputId": "e40620bf-d7c2-4d60-d957-17a58c2d0b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM-o8ncjoTOz",
        "outputId": "7c34d052-d528-47d1-c92f-60bcceb49d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-core==0.1.37\n",
            "  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (0.1.147)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.1.37) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.1.37) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.1.37) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.1.37) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.1.37) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-core==0.1.37) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.1.37) (1.3.1)\n",
            "Downloading langchain_core-0.1.37-py3-none-any.whl (274 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/274.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.53\n",
            "    Uninstalling langchain-core-0.1.53:\n",
            "      Successfully uninstalled langchain-core-0.1.53\n",
            "Successfully installed langchain-core-0.1.37\n",
            "Collecting langchain-groq==0.1.1\n",
            "  Downloading langchain_groq-0.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq==0.1.1) (0.19.0)\n",
            "Collecting langchain-core<0.2.0,>=0.1.41 (from langchain-groq==0.1.1)\n",
            "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.1) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (0.1.147)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (23.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq==0.1.1) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.41->langchain-groq==0.1.1) (2.3.0)\n",
            "Downloading langchain_groq-0.1.1-py3-none-any.whl (11 kB)\n",
            "Using cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "Installing collected packages: langchain-core, langchain-groq\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.37\n",
            "    Uninstalling langchain-core-0.1.37:\n",
            "      Successfully uninstalled langchain-core-0.1.37\n",
            "Successfully installed langchain-core-0.1.53 langchain-groq-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core==0.1.37\n",
        "!pip install langchain-groq==0.1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM2zAVzIXGxX",
        "outputId": "f43785d9-bddb-45bc-e3fb-697827988d66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "# Set GPU device\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
        "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2qt6JHCgMFf",
        "outputId": "8d3b0b6b-f7eb-4b60-ef54-66e4e69fcea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "11.7\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())  # Should print True\n",
        "print(torch.version.cuda)  # Check CUDA version\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stJ141lXX3Ig",
        "outputId": "62a41144-1eda-4142-c6de-f4991bfb3d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PHUE  PWINT  THWE   No.(164B),  Pinya  10\n",
            " th  \n",
            "Steet,  Block  5,  South  Okkalapa  Township,  Yangon  *  (+95)  09422201502  *  ppthwe99@gmail.com  \n",
            "PERSONAL\n",
            " \n",
            "PROFILE\n",
            "  \n",
            " \n",
            "I  am  a  27-year-old  professional  with  a  Bachelor  of  Engineering  in  Electronics  from  Technological  University  \n",
            "(Dawei).\n",
            " \n",
            "I\n",
            " \n",
            "have\n",
            " \n",
            "over\n",
            " \n",
            "3\n",
            " \n",
            "years\n",
            " \n",
            "of\n",
            " \n",
            "work\n",
            " \n",
            "experience\n",
            " \n",
            "in\n",
            " \n",
            "the\n",
            " \n",
            "IT\n",
            " \n",
            "and\n",
            " \n",
            "telecommunications\n",
            " \n",
            "industry,\n",
            " \n",
            "particularly\n",
            " \n",
            "in\n",
            " \n",
            "network\n",
            " \n",
            "engineering\n",
            " \n",
            "and\n",
            " \n",
            "IT\n",
            " \n",
            "support\n",
            " \n",
            "roles\n",
            " \n",
            "within\n",
            " \n",
            "financial\n",
            " \n",
            "and\n",
            " \n",
            "ISP\n",
            " \n",
            "sectors.\n",
            " \n",
            "Currently,\n",
            " \n",
            "I\n",
            " \n",
            "am\n",
            " \n",
            "pursuing\n",
            " \n",
            "a\n",
            " \n",
            "Master’s\n",
            " \n",
            "degree\n",
            " \n",
            "at\n",
            " \n",
            "AIT\n",
            " \n",
            "University,\n",
            " \n",
            "focusing\n",
            " \n",
            "on\n",
            " \n",
            "Data\n",
            " \n",
            "Science\n",
            " \n",
            "and\n",
            " \n",
            "Artificial\n",
            " \n",
            "Intelligence.\n",
            " \n",
            "My\n",
            " \n",
            "research\n",
            " \n",
            "interests\n",
            " \n",
            "lie\n",
            " \n",
            "in\n",
            " \n",
            "network\n",
            " \n",
            "security,\n",
            " \n",
            "AI-driven\n",
            " \n",
            "automation,\n",
            " \n",
            "and\n",
            " \n",
            "digital\n",
            " \n",
            "transformation.\n",
            " \n",
            "I\n",
            " \n",
            "am\n",
            " \n",
            "particularly\n",
            " \n",
            "interested\n",
            " \n",
            "in\n",
            " \n",
            "exploring\n",
            " \n",
            "how\n",
            " \n",
            "cultural\n",
            " \n",
            "values\n",
            " \n",
            "influence\n",
            " \n",
            "technological\n",
            " \n",
            "advancements\n",
            " \n",
            "and\n",
            " \n",
            "ensuring\n",
            " \n",
            "that\n",
            " \n",
            "AI\n",
            " \n",
            "systems\n",
            " \n",
            "are\n",
            " \n",
            "developed\n",
            " \n",
            "with\n",
            " \n",
            "ethical\n",
            " \n",
            "considerations.\n",
            " \n",
            "My\n",
            " \n",
            "professional\n",
            " \n",
            "journey\n",
            " \n",
            "has\n",
            " \n",
            "strengthened\n",
            " \n",
            "my\n",
            " \n",
            "technical\n",
            " \n",
            "problem-solving\n",
            " \n",
            "skills,\n",
            " \n",
            "data\n",
            " \n",
            "analysis\n",
            " \n",
            "expertise,\n",
            " \n",
            "and\n",
            " \n",
            "ability\n",
            " \n",
            "to\n",
            " \n",
            "work\n",
            " \n",
            "in\n",
            " \n",
            "fast-paced\n",
            " \n",
            "IT\n",
            " \n",
            "environments.\n",
            " \n",
            "I\n",
            " \n",
            "am\n",
            " \n",
            "passionate\n",
            " \n",
            "about\n",
            " \n",
            "leveraging\n",
            " \n",
            "AI\n",
            " \n",
            "and\n",
            " \n",
            "technology\n",
            " \n",
            "to\n",
            " \n",
            "drive\n",
            " \n",
            "innovation\n",
            " \n",
            "and\n",
            " \n",
            "efficiency\n",
            " \n",
            "in\n",
            " \n",
            "the\n",
            " \n",
            "financial\n",
            " \n",
            "and\n",
            " \n",
            "communication\n",
            " \n",
            "sectors.\n",
            " \n",
            "EDUCATION  BACKGROUND   \n",
            "12/2013  –  11/2017  Technological  University  (Dawei)   Bachelor  of  Technology  (Electronics).    \n",
            "12/2017  –  11/2019  Technological  University  (Dawei)   Bachelor  of  Engineering  (Electronics).    Thesis:  ANALYSIS  OF  SIGNAL  PERFORMANCE  BASED  ON  OPTICAL  LIGHT  SOURCE   \n",
            "IN\n",
            " \n",
            "FIBER-TO-THE-HOME\n",
            " \n",
            "(FTTH)\n",
            " \n",
            "NETWORK\n",
            " \n",
            " \n",
            "PROFESSIONAL  EXPERIENCE   \n",
            "04/2022  –  02/2023  NOC  &  Service  Desk  Engineer   Kan  Baw  Za  Company  Ltd.,  [KBZ  Bank]   \n",
            "•\n",
            " \n",
            "Monitor  all  critical  IT  objects  in   \n",
            "•\n",
            " \n",
            "Monitor  Core  Network  Infra,  network  traffics,  network  status  and  server   \n",
            "•\n",
            " \n",
            "Adhering  to  Service  Level  Agreements  (SLAs)   \n",
            "•\n",
            " \n",
            "Basic  Incident  Management  and  Problem  Management   \n",
            "•\n",
            " \n",
            "Provide  level  1  support  and  pass  incident  ownership  if  escalation  is  needed   \n",
            "•\n",
            " \n",
            "Good  knowledge  in  VPN,  One  Login,  RSA  SecureID    \n",
            "•\n",
            " \n",
            "Prepare  daily,  weekly  and  monthly  reports  to  related  team   \n",
            "•\n",
            " \n",
            "Administrating  on  Window  OS,  AD  and  Meraki  AP   \n",
            "•\n",
            " \n",
            "Remote  support  to  end  user’s  PC  and  Laptop   \n",
            "•\n",
            " \n",
            "Good  understanding  of  OSI  and  TCP/IP   \n",
            "•\n",
            " \n",
            "Service  Now  and  Genesys  Ticketing  Software   \n",
            "12/2019  –  01/2021  Customer  Service  Engineer  (FTTH)   MKTN  Fiber  Network  Consulting  and  Solutions  Provider   \n",
            "•\n",
            " \n",
            "Troubleshooting  about  Local  Area  Network  and  routers   \n",
            "•\n",
            " \n",
            "Configure  Huawei  and  ZTE  GPON  ONU  used  for  optical  transmission   \n",
            "•\n",
            " \n",
            "FTTH  customer  service  and  complaints  handling  about  ISP   \n",
            "•\n",
            " \n",
            "Prepared  reports  and  documents  to  ensure  smooth  operations    \n",
            "•\n",
            " \n",
            "Go  and  Check  Sites  according  to  reports    \n",
            "•\n",
            " \n",
            "Follow  up  to  customer  with  open  ticket  LANGUAGES   \n",
            "English  Intermediate    Burmese  Native   \n",
            "SKILLS   \n",
            "EnGenius  Certified  Network  Specialist  Fortinet  Certified  Fundamentals  Cybersecurity  Zabbix  and  eG  Monitoring  Tools    Service  Now   Genesys  Ticketing  System    Microsoft  Office  (Word,Excel,Power  Point)   \n",
            "PERSONAL  STRENGTHS   \n",
            "Time  management    Fast  learner  and  physically  fit    Active  as  well  as  an  independent  team  player  Careful  and  Meticulous    Strong  Multicultural  awareness   \n",
            "CORE  BELIEFS  &  TECHNOLOGY  VISION  \n",
            "I  believe  that  technology  plays  a  crucial  role  in  shaping  society  by  driving  economic  growth,  digital  inclusion,  and  security  advancements.  However,  I  strongly  advocate  for  ethical  AI  practices  that  consider  cultural  values  and  local  contexts  to  prevent  bias  in  data-driven  decision-making.  As  a  Master's  student  in  Data  Science  &  AI,  I  aim  to  contribute  to  responsible  technological  innovations  that  enhance  financial  security  and  IT  infrastructure  while  ensuring  fairness  and  transparency  in  AI  systems.  \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the PDF file\n",
        "pdf_path = \"/content/drive/MyDrive/Colab_Notebooks/PHUE PWINT THWE Academic CV (1).pdf\"  # Adjust path if needed\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Display extracted text for verification\n",
        "for doc in documents[:3]:  # Display first 3 extracted text parts\n",
        "    print(doc.page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SnycUJZYW8p",
        "outputId": "0c76bb5d-d3e5-4ffc-b98d-1c9e87db7c5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycI-_1S0Yc6n",
        "outputId": "b01713b8-c54d-425a-a583-bc218746cf71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"•\\n \\nTroubleshooting  about  Local  Area  Network  and  routers   \\n•\\n \\nConfigure  Huawei  and  ZTE  GPON  ONU  used  for  optical  transmission   \\n•\\n \\nFTTH  customer  service  and  complaints  handling  about  ISP   \\n•\\n \\nPrepared  reports  and  documents  to  ensure  smooth  operations    \\n•\\n \\nGo  and  Check  Sites  according  to  reports    \\n•\\n \\nFollow  up  to  customer  with  open  ticket  LANGUAGES   \\nEnglish  Intermediate    Burmese  Native   \\nSKILLS   \\nEnGenius  Certified  Network  Specialist  Fortinet  Certified  Fundamentals  Cybersecurity  Zabbix  and  eG  Monitoring  Tools    Service  Now   Genesys  Ticketing  System    Microsoft  Office  (Word,Excel,Power  Point)   \\nPERSONAL  STRENGTHS   \\nTime  management    Fast  learner  and  physically  fit    Active  as  well  as  an  independent  team  player  Careful  and  Meticulous    Strong  Multicultural  awareness   \\nCORE  BELIEFS  &  TECHNOLOGY  VISION  \\nI  believe  that  technology  plays  a  crucial  role  in  shaping  society  by  driving  economic  growth,  digital  inclusion,  and  security  advancements.  However,  I  strongly  advocate  for  ethical  AI  practices  that  consider  cultural  values  and  local  contexts  to  prevent  bias  in  data-driven  decision-making.  As  a  Master's  student  in  Data  Science  &  AI,  I  aim  to  contribute  to  responsible  technological  innovations  that  enhance  financial  security  and  IT  infrastructure  while  ensuring  fairness  and  transparency  in  AI  systems.  \\n \\n\", metadata={'source': '/content/drive/MyDrive/Colab_Notebooks/PHUE PWINT THWE Academic CV (1).pdf', 'page': 1})"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJpDMfJmYD_N",
        "outputId": "5f843264-c931-4bf9-c48e-e541bc99c406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Chunks: 61\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,  # Chunk size of 100 characters\n",
        "    chunk_overlap= 20  # 25-character overlap\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Total Chunks: {len(chunks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554,
          "referenced_widgets": [
            "f3575f672da04e5196e36f0392167aa2",
            "3c1088214f954616926a66e2b0272cc0",
            "9cd2f470abe44e36b0bb596228ed73d4",
            "03187ace2ae14476b6c33103a0b7ce52",
            "243f7cd7f50d472bbe8ec75d6d02f0b3",
            "312ed65f33434b11b126ef21ecf3132b",
            "dbf493dbdddd4f63b40ab87cedf4bc76",
            "6122dccb0ca84103861c416b1a4fcdf3",
            "abbaab5b3c064b89bb1187c7a2d1070f",
            "a0ade36a76964fa1aa160377706a861a",
            "5cfd19672d8546a2b4df2114651f2d3c",
            "5610ac85e2cf45dc84b1978cf2318898",
            "6172699e6f784e6f9b5a7ef071df159f",
            "51cd996d1d6c4a078813c264ca243c97",
            "a335fa3fd6b2452780a92a2aa7cb52e4",
            "cc76b2e3cdf744f9a4345b3d6242004c",
            "da2a7f7693544e58b2f32ffae1fd57e6",
            "fefc116b78804bf8a77c586cf9fce6d8",
            "c7e808292ac44d0bb5804480c2f68070",
            "37c653b9a965423cbb7828aa43baf75e",
            "99a8cc9ed7db427dae73f535d9f73922",
            "e7ba6ffa69a54f8cb26ab6aa55cc610a",
            "22e49617f43c432d856e2d42afcae380",
            "bc4644495fa34a8ab0314ef91d7a1e30",
            "9edc0f613dce43c694170d3a2d21ea3c",
            "083212ca1ccf4d18aaa41a49eae482dc",
            "aba0492d46d340e3a23f800b776d12ee",
            "ee9868ce97ad4ddb9eae445e36149066",
            "46ecf6d5b2364948857e408e952dc877",
            "23e067a378164f60b753ce6f1710e3ab",
            "53b88c5dc76c4c42b18d9f7d935cf655",
            "92c22228eac94149ae883773ac2dcc67",
            "7d4923f33e794ecaa228592631861305",
            "8fcba16f63af4b2995b44fef801e7fbd",
            "c63afc7cf86d4d98a953ee3fdf0b369e",
            "057931756a574bb6b8fdf83e9f318b04",
            "d8def0f8dfa94378bb0f7a0ed547fd27",
            "49fae5bfc0704f6eb74aab20735c2c3b",
            "dfdafcdf084f42b9bb3c02e41eeb51cf",
            "cfda8d5b6426496ab6060a1f4db39af9",
            "4ae1ff3857694a0196319d2868da5570",
            "1eb927dcb0434deaa06071830334b438",
            "47cfd264cc5a4f70a9852e7fa0a5e6fe",
            "1eacc1b9159648529f36766bcfc927c8",
            "873731292e6b40228764055852266c9c",
            "d708b5c91b904459a37870128b88b33a",
            "64cfe274e3274f89ba5aa78540bb86af",
            "716f2db1a197491ab8ecfd17f2d276ee",
            "d0727b41658e458da886122b6ff3c51a",
            "b51c68a1e39a46b88fb4927457165a1f",
            "bc90b619bc154ca2b4e43df89dfd369a",
            "fada8250faf946d2a527de651a16f304",
            "3440fb6b29ea4dd796bcfcf749f156c5",
            "120c64db30a34386926cceade05be14f",
            "a978989f91fb49789d9e93facb381727",
            "fe17eaa65b6f45a7be958080d22c881d",
            "061091dab5c044e3a7f262c06356e720",
            "aa2e011799c94c0e9ceb33f496337499",
            "e50d12e74a3944c5a3276b427d8b9769",
            "dc997a8f380a490c87b501a5600c1d0b",
            "7ae93f56ecf142c2b28f5e2b4d2aee58",
            "05aa013bc1b4459bb88e677c6a5eb2e3",
            "2e53e3c07858449da0187fd05d859017",
            "6a3475fe748c4003a401ca0f6e2baf3c",
            "e85dd7dda34943b6a0822de3f8646da5",
            "97d965131a0c447486af9d981e4b7b76",
            "e3e9be76ab0e4f91a3ecc34f0f038631",
            "df2ffbe7a57b431880dee906a70fa785",
            "271f8a1080184ca5988b6ec45e9cce0c",
            "e80eff8576c849df992ff15c4f4641f6",
            "2d7b9fc5ebc445fc99186d0673af8b4f",
            "aa6a485cbee1487ab1f4bb6a5cb640eb",
            "9736fc23961a4227b0ca20c5e68e6437",
            "82a2d495b5b5458bb260a514bf07fb43",
            "dc5b1785c3b84b5380bf628201b7c645",
            "2a3c9352d9c04817bcf7089fa67ba34f",
            "c48a11b8177c403eb61ec4d40c62c974",
            "14702bce773c46a4b367de6c6b3f5139",
            "cc165f194f7c4c4f9ac2f564b626ca9a",
            "2711727bcdb747bcbbdf0a6b6579fcfa",
            "bd66c79cf7044338b33c9b2f72c2b1dc",
            "ca5ad3c29a204c178ff9fe77e3196834",
            "e6a6432491074767aae72160e7d39a1d",
            "58fceb527e7143bc967fe426d0f91bf0",
            "f1b8fcebbcd94881b80b45ca0afba327",
            "d976dbd11f5e4e4d8ff4f9b9b07fd2af",
            "e5934b2790694ca9882792785006f4b9",
            "8a0da4680661447594226a85b33ba154",
            "b6bc10e431d345b1a23dc3e3b9c88d12",
            "b20834e68b4e49578e0733e3f0a9b867",
            "9f716d5d3fc941218634ec9bb0384ca7",
            "be26253cdd67426e93241ff7d172e157",
            "1056c4584ba44735bafa8029b905f5f5",
            "266cf3c0ed444c4493eba2054469c74e",
            "912ec7d4aae543549e3fd69ec4bce140",
            "b07de924f11043bfbb4aaa8e8042cb99",
            "e287e3e392b14dc68d97ae1f8097431a",
            "63110cba947d48b9931dbf03fe0cb354",
            "606562365d6d47118a9fbe26be605539",
            "f9655cb71af6478d82c35409e2bdd81c",
            "2baaa45beed14fe18057fa2b83d4df7e",
            "91237860b841413298b654602f308298",
            "ff4cf821963b4eb9a872eeffb4fb60e0",
            "94fb85d5bb8f401a827b28733d2f30fe",
            "32c59bcb246540348deb876e8c4127e2",
            "0d36ead40d704b54a504db6688e7c364",
            "e9bb8b32b6184137a2e87e842e942240",
            "ff0a780283644b0caab8f1040bef7a83",
            "dd72728d7c034146b3d6f4909f7a0778",
            "556b19bec5224a868f332beb71f5cfdc",
            "be35c939829242c7bf00e46e4a38bdd4",
            "b9c662c8eeaf44928974542b501c7401",
            "2f9d156e7cff4d3dbbad44a67d568c70",
            "c42a5adbae174eb9b605cbe3d3acc0c2",
            "6ebafc8e3bf4427ab8ba09b6d0742843",
            "01a0fa8a4c9849378ca36d86502ae286",
            "6129f2bc7f2349d7930a5c86f21aa938",
            "3dc139e4670b4d79a60bdc71dcbcbbe4",
            "d657c1894e4f47508318add12663ebb6",
            "fe00ce45ecaf414d8945db61461d8846",
            "5c8dbf745dc64d35a844e48bd1b8dd8b",
            "a8113dfe64c848108a834dc60e188fc1",
            "8e3be967292e4100b5ec53356c22c298",
            "a1be472f746b4274b6141707886349b2",
            "cbee5d7444434b479f314ea534bf8ecf",
            "f6f04e4da07f45d8b8847c703df4a5e6",
            "366afa6daa0244d2a9ab60fa98e1da76",
            "7d0057b98f044605b26a70a59a80a0e4",
            "321da4eadbb0460590ecc6f1fa7ff6a3",
            "e0d3539c0914464ea1097d4c2c3baa54",
            "4828a17c698849dcbf25291c4825624d",
            "3e3e2f74735848a4b91e84d5b97c690f",
            "a11cf2bdf30a49cbbfa11a386461a407",
            "554cfabb1dea49c4bb10d27d76aa6c93",
            "7ee814d0805046d6b7b9679b52386be9",
            "cee3785ab48f47ee9e9952a6ce420d0c",
            "7f8f2f50c27b46a2904871d246d0e187",
            "6630f4fa37e3488c858b4a6ae0623893",
            "f83295ee00ca45bf9469de5cffd080ec",
            "47ee8e0f5dc3484e979d613f33256615",
            "997240772c3c46e38cf92b1899dcf61c",
            "6bf95547aa314324b16acd01c4d63ad0",
            "b41ea69dc06a4b30bbbb4e4b76f0d0e4",
            "f860fb31236c4d5ab571b2e5016ad68b",
            "cc93a5fff21a4d93a66422e58ee9fe28",
            "c59ce74d714544308864857ee4343b0b",
            "706e124f80674223945d6324d281b6c6",
            "1dd05dae35494607a1e735bf4d6ed11e",
            "c9a871dea8b0402f82ca5561e4af4647",
            "43c70c25fcaa447cbb419583ef5c0d4c",
            "cbd3c6c27a224085a214cdc5c8db0dd9",
            "1802ae8619ff4f2abdb868934be4b357",
            "2599e54b10a44a5cb627f8e58aa4e901",
            "ca92559259b641b5915e6ee29b8760e5"
          ]
        },
        "id": "CfePq7mVYPxi",
        "outputId": "7401c7f1-e798-4256-f9e4-2191c32944bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3575f672da04e5196e36f0392167aa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading .gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5610ac85e2cf45dc84b1978cf2318898",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22e49617f43c432d856e2d42afcae380",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fcba16f63af4b2995b44fef801e7fbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "873731292e6b40228764055852266c9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading README.md:   0%|          | 0.00/66.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe17eaa65b6f45a7be958080d22c881d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3e9be76ab0e4f91a3ecc34f0f038631",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14702bce773c46a4b367de6c6b3f5139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6bc10e431d345b1a23dc3e3b9c88d12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9655cb71af6478d82c35409e2bdd81c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be35c939829242c7bf00e46e4a38bdd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8113dfe64c848108a834dc60e188fc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a11cf2bdf30a49cbbfa11a386461a407",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f860fb31236c4d5ab571b2e5016ad68b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "model_name = 'hkunlp/instructor-base'\n",
        "\n",
        "embedding_model = HuggingFaceInstructEmbeddings(\n",
        "    model_name = model_name,\n",
        "    model_kwargs = {\"device\" : device}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "eI7hICxTYjr1"
      },
      "outputs": [],
      "source": [
        "#save vector locally\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "vector_path = '/content/drive/MyDrive/Colab_Notebooks/vector-store'\n",
        "\n",
        "vectordb = FAISS.from_documents(\n",
        "    documents = chunks,\n",
        "    embedding = embedding_model\n",
        ")\n",
        "\n",
        "db_file_name = 'Phue'\n",
        "\n",
        "vectordb.save_local(\n",
        "    folder_path = os.path.join(vector_path, db_file_name),\n",
        "    index_name = 'PPT' #default index\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlVfeNe1Y_3h",
        "outputId": "97f3d205-2350-489d-c05b-b7a174b5f336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FAISS index loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#calling vector from local\n",
        "vector_path = '/content/drive/MyDrive/Colab_Notebooks/vector-store'\n",
        "db_file_name = 'Phue'\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vectordb = FAISS.load_local(\n",
        "    folder_path=os.path.join(vector_path, db_file_name),\n",
        "    embeddings=embedding_model,\n",
        "    index_name='PPT',\n",
        "    allow_dangerous_deserialization=True  # ✅ Enable safe deserialization\n",
        ")\n",
        "\n",
        "print(\"✅ FAISS index loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4Nko15XAZpwC"
      },
      "outputs": [],
      "source": [
        "\n",
        "#ready to use\n",
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fXj0CH0ZhRc",
        "outputId": "a9555736-e8b2-457f-ba99-744fcce6508d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='EDUCATION BACKGROUND   \\n \\n12/2013 – 11/2017 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}),\n",
              " Document(page_content='career and personal development.  Looking ahead to advance knowledge gain during the study period', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}),\n",
              " Document(page_content='the study period and initiate', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}),\n",
              " Document(page_content='Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "retriever.get_relevant_documents(\"education?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDYy__bmZnks",
        "outputId": "3c83b256-ffa7-4531-d617-56269ecf8c90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='LANGUAGES \\n \\nEnglish  Intermediate  \\nBurmese  Native \\n \\n \\nSKILLS', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 1}),\n",
              " Document(page_content='Strong Multi-culture awareness', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 1}),\n",
              " Document(page_content='(+95) 09422201502 * ppthwe99@gmail.com', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}),\n",
              " Document(page_content='PHUE PWINT THWE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "retriever.get_relevant_documents(\"language?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "nLSLS0CGZw0n"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are an AI assistant. Answer the following question based on the provided context\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Gentle & Informative Answer:\n",
        "\"\"\".strip()\n",
        "\n",
        "# Load Prompt Template\n",
        "PROMPT = PromptTemplate.from_template(prompt_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xm4m0JVaE2H",
        "outputId": "ba7f44b1-3d2d-4d99-a90a-36533107b865"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
        "from langchain.llms.base import LLM\n",
        "from pydantic.v1 import BaseModel, Extra\n",
        "import torch\n",
        "\n",
        "# Model setup\n",
        "model_id = '/content/drive/MyDrive/Colab_Notebooks/A6_models/fastchat-t5-3b-v1.0/'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "bitsandbyte_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bitsandbyte_config,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0,\n",
        "    repetition_penalty=1.5\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wJmGn-dxEscS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pydantic.v1 import BaseModel, Extra\n",
        "\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Wrap Model for LangChain\n",
        "class CustomHuggingFacePipeline(LLM, BaseModel):\n",
        "    pipeline: object\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.allow\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _call(self, prompt: str, stop=None, **kwargs):\n",
        "        response = self.pipeline(prompt, max_new_tokens=256, temperature=0, repetition_penalty=1.5)\n",
        "        return response[0][\"generated_text\"]\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self):\n",
        "        return \"custom_transformers\"\n",
        "\n",
        "llm = CustomHuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "aq1YkFQjEv4b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#  Question Generator (Rewrites User Queries)\n",
        "question_generator = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=CONDENSE_QUESTION_PROMPT,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "#  Retrieval Chain (Fetches Relevant Documents)\n",
        "doc_chain = load_qa_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    prompt=PROMPT,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "#  Conversational Memory\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    k=3,\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "#  Conversational Retrieval Chain\n",
        "Chain = ConversationalRetrievalChain(\n",
        "    retriever=retriever,  # FAISS Retriever\n",
        "    question_generator=question_generator,\n",
        "    combine_docs_chain=doc_chain,\n",
        "    return_source_documents=True,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76FqAZzlDWHl",
        "outputId": "86bf9f73-0e40-49d5-c5f5-43c8a961a6f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "PHUE PWINT THWE\n",
            "\n",
            "(+95) 09422201502 * ppthwe99@gmail.com\n",
            "\n",
            "EDUCATION BACKGROUND   \n",
            " \n",
            "12/2013 – 11/2017 Technological University (Dawei)\n",
            "\n",
            "Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.\n",
            "\n",
            "Question: Who is Phue Pwint Thwe?\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer: {'question': 'Who is Phue Pwint Thwe?', 'chat_history': [], 'answer': '<pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\\n', 'source_documents': [Document(page_content='PHUE PWINT THWE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='(+95) 09422201502 * ppthwe99@gmail.com', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='EDUCATION BACKGROUND   \\n \\n12/2013 – 11/2017 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Test the Chatbot\n",
        "prompt_question = \"Who is Phue Pwint Thwe?\"\n",
        "answer = Chain({\"question\": prompt_question})\n",
        "print(\"Bot Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxcTxP_VNsRO",
        "outputId": "157e90e3-ca2b-46b1-e2cc-57d3d05393cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Who is Phue Pwint Thwe?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\n",
            "\n",
            "Follow Up Input: How old are you?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "(+95) 09422201502 * ppthwe99@gmail.com\n",
            "\n",
            "PERSONAL PROFILE\n",
            "\n",
            "the study period and initiate\n",
            "\n",
            "Active as well as an independent team player \n",
            " Careful and Meticulous\n",
            "\n",
            "Question: <pad> How  old  are  you?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How old are you?': {'question': 'How old are you?', 'chat_history': [HumanMessage(content='Who is Phue Pwint Thwe?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\\n')], 'answer': '<pad> I  am  an  AI  assistant,  so  I  do  not  have  personal  information.\\n', 'source_documents': [Document(page_content='(+95) 09422201502 * ppthwe99@gmail.com', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='PERSONAL PROFILE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='the study period and initiate', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Active as well as an independent team player \\n Careful and Meticulous', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 1})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Who is Phue Pwint Thwe?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: <pad> I  am  an  AI  assistant,  so  I  do  not  have  personal  information.\n",
            "\n",
            "Follow Up Input: What is your highest level of education?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "PHUE PWINT THWE\n",
            "\n",
            "EDUCATION BACKGROUND   \n",
            " \n",
            "12/2013 – 11/2017 Technological University (Dawei)\n",
            "\n",
            "Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.\n",
            "\n",
            "(+95) 09422201502 * ppthwe99@gmail.com\n",
            "\n",
            "Question: <pad> What  is  Phue  Pwint  Thwe's  highest  level  of  education?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What is your highest level of education?': {'question': 'What is your highest level of education?', 'chat_history': [HumanMessage(content='Who is Phue Pwint Thwe?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\\n'), HumanMessage(content='How old are you?'), AIMessage(content='<pad> I  am  an  AI  assistant,  so  I  do  not  have  personal  information.\\n')], 'answer': \"<pad> Phue  Pwint  Thwe's  highest  level  of  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei),  with  a  specialization  in  Electronics.\\n\", 'source_documents': [Document(page_content='PHUE PWINT THWE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='EDUCATION BACKGROUND   \\n \\n12/2013 – 11/2017 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='(+95) 09422201502 * ppthwe99@gmail.com', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Who is Phue Pwint Thwe?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: <pad> I  am  an  AI  assistant,  so  I  do  not  have  personal  information.\n",
            "\n",
            "Human: What is your highest level of education?\n",
            "Assistant: <pad> Phue  Pwint  Thwe's  highest  level  of  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei),  with  a  specialization  in  Electronics.\n",
            "\n",
            "Follow Up Input: What major or field of study did you pursue during your education?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "PHUE PWINT THWE\n",
            "\n",
            "Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.\n",
            "\n",
            "EDUCATION BACKGROUND   \n",
            " \n",
            "12/2013 – 11/2017 Technological University (Dawei)\n",
            "\n",
            "my advanced career opportunity with the postgraduate research at AIT University.\n",
            "\n",
            "Question: <pad> What  was  Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What major or field of study did you pursue during your education?': {'question': 'What major or field of study did you pursue during your education?', 'chat_history': [HumanMessage(content='Who is Phue Pwint Thwe?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\\n'), HumanMessage(content='How old are you?'), AIMessage(content='<pad> I  am  an  AI  assistant,  so  I  do  not  have  personal  information.\\n'), HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"<pad> Phue  Pwint  Thwe's  highest  level  of  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei),  with  a  specialization  in  Electronics.\\n\")], 'answer': \"<pad> Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education  was  Electronics.\\n\", 'source_documents': [Document(page_content='PHUE PWINT THWE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='EDUCATION BACKGROUND   \\n \\n12/2013 – 11/2017 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='my advanced career opportunity with the postgraduate research at AIT University.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How old are you?\n",
            "Assistant: <pad> I  am  an  AI  assistant,  so  I  do  not  have  personal  information.\n",
            "\n",
            "Human: What is your highest level of education?\n",
            "Assistant: <pad> Phue  Pwint  Thwe's  highest  level  of  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei),  with  a  specialization  in  Electronics.\n",
            "\n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: <pad> Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education  was  Electronics.\n",
            "\n",
            "Follow Up Input: How many years of work experience do you have?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "PHUE PWINT THWE\n",
            "\n",
            "IN FIBER-TO-THE-HOME (FTTH) NETWORK \n",
            " \n",
            "PROFESSIONAL EXPERIENCE\n",
            "\n",
            "(+95) 09422201502 * ppthwe99@gmail.com\n",
            "\n",
            "my advanced career opportunity with the postgraduate research at AIT University.\n",
            "\n",
            "Question: <pad>  How  many  years  of  work  experience  does  Phue  Pwint  Thwe  have?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How many years of work experience do you have?': {'question': 'How many years of work experience do you have?', 'chat_history': [HumanMessage(content='How old are you?'), AIMessage(content='<pad> I  am  an  AI  assistant,  so  I  do  not  have  personal  information.\\n'), HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"<pad> Phue  Pwint  Thwe's  highest  level  of  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei),  with  a  specialization  in  Electronics.\\n\"), HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"<pad> Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education  was  Electronics.\\n\")], 'answer': '<pad> Phue  Pwint  Thwe  has  a  professional  experience  in  Fiber-To-The-Home  (FTTH)  Network  Professional  Experience,  but  the  information  provided  does  not  specify  how  many  years  of  work  experience  she  has.\\n', 'source_documents': [Document(page_content='PHUE PWINT THWE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='IN FIBER-TO-THE-HOME (FTTH) NETWORK \\n \\nPROFESSIONAL EXPERIENCE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='(+95) 09422201502 * ppthwe99@gmail.com', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='my advanced career opportunity with the postgraduate research at AIT University.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What is your highest level of education?\n",
            "Assistant: <pad> Phue  Pwint  Thwe's  highest  level  of  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei),  with  a  specialization  in  Electronics.\n",
            "\n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: <pad> Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education  was  Electronics.\n",
            "\n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  has  a  professional  experience  in  Fiber-To-The-Home  (FTTH)  Network  Professional  Experience,  but  the  information  provided  does  not  specify  how  many  years  of  work  experience  she  has.\n",
            "\n",
            "Follow Up Input: What type of work or industry have you been involved in?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "in Data Science and Artificial Intelligence System after working in the IT field of financial\n",
            "\n",
            "career and personal development.  Looking ahead to advance knowledge gain during the study period\n",
            "\n",
            "IN FIBER-TO-THE-HOME (FTTH) NETWORK \n",
            " \n",
            "PROFESSIONAL EXPERIENCE\n",
            "\n",
            "my advanced career opportunity with the postgraduate research at AIT University.\n",
            "\n",
            "Question: <pad>  What  type  of  work  or  industry  have  you  been  involved  in?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What type of work or industry have you been involved in?': {'question': 'What type of work or industry have you been involved in?', 'chat_history': [HumanMessage(content='What is your highest level of education?'), AIMessage(content=\"<pad> Phue  Pwint  Thwe's  highest  level  of  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei),  with  a  specialization  in  Electronics.\\n\"), HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"<pad> Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education  was  Electronics.\\n\"), HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='<pad> Phue  Pwint  Thwe  has  a  professional  experience  in  Fiber-To-The-Home  (FTTH)  Network  Professional  Experience,  but  the  information  provided  does  not  specify  how  many  years  of  work  experience  she  has.\\n')], 'answer': '<pad> Phue  Pwint  Thwe  has  been  involved  in  the  IT  field,  specifically  in  financial  career  and  personal  development.\\n', 'source_documents': [Document(page_content='in Data Science and Artificial Intelligence System after working in the IT field of financial', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='career and personal development.  Looking ahead to advance knowledge gain during the study period', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='IN FIBER-TO-THE-HOME (FTTH) NETWORK \\n \\nPROFESSIONAL EXPERIENCE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='my advanced career opportunity with the postgraduate research at AIT University.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What major or field of study did you pursue during your education?\n",
            "Assistant: <pad> Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education  was  Electronics.\n",
            "\n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  has  a  professional  experience  in  Fiber-To-The-Home  (FTTH)  Network  Professional  Experience,  but  the  information  provided  does  not  specify  how  many  years  of  work  experience  she  has.\n",
            "\n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  has  been  involved  in  the  IT  field,  specifically  in  financial  career  and  personal  development.\n",
            "\n",
            "Follow Up Input: Can you describe your current role or job responsibilities?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "• Prepare daily, weekly and monthly reports to related team\n",
            "\n",
            "12/2019 – 01/2021 Customer Service Engineer (FTTH)\n",
            "\n",
            "Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community\n",
            "\n",
            "my advanced career opportunity with the postgraduate research at AIT University.\n",
            "\n",
            "Question: <pad>  Can  you  describe  your  current  role  or  job  responsibilities?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'Can you describe your current role or job responsibilities?': {'question': 'Can you describe your current role or job responsibilities?', 'chat_history': [HumanMessage(content='What major or field of study did you pursue during your education?'), AIMessage(content=\"<pad> Phue  Pwint  Thwe's  major  or  field  of  study  during  his/her  education  was  Electronics.\\n\"), HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='<pad> Phue  Pwint  Thwe  has  a  professional  experience  in  Fiber-To-The-Home  (FTTH)  Network  Professional  Experience,  but  the  information  provided  does  not  specify  how  many  years  of  work  experience  she  has.\\n'), HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='<pad> Phue  Pwint  Thwe  has  been  involved  in  the  IT  field,  specifically  in  financial  career  and  personal  development.\\n')], 'answer': '<pad> Phue  Pwint  Thwe  is  currently  a  Customer  Service  Engineer  (FTTH)  with  the  postgraduate  research  opportunity  at  AIT  University.  In  this  role,  she  provides  technical  support  and  troubleshooting  to  customers  using  fiber-optic  technology.  She  also  prepares  daily,  weekly,  and  monthly  reports  for  her  related  team.\\n', 'source_documents': [Document(page_content='• Prepare daily, weekly and monthly reports to related team', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='12/2019 – 01/2021 Customer Service Engineer (FTTH)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='my advanced career opportunity with the postgraduate research at AIT University.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: How many years of work experience do you have?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  has  a  professional  experience  in  Fiber-To-The-Home  (FTTH)  Network  Professional  Experience,  but  the  information  provided  does  not  specify  how  many  years  of  work  experience  she  has.\n",
            "\n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  has  been  involved  in  the  IT  field,  specifically  in  financial  career  and  personal  development.\n",
            "\n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  currently  a  Customer  Service  Engineer  (FTTH)  with  the  postgraduate  research  opportunity  at  AIT  University.  In  this  role,  she  provides  technical  support  and  troubleshooting  to  customers  using  fiber-optic  technology.  She  also  prepares  daily,  weekly,  and  monthly  reports  for  her  related  team.\n",
            "\n",
            "Follow Up Input: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "Bachelor of Technology (Electronics).  \n",
            " \n",
            "12/2017 – 11/2019 Technological University (Dawei)\n",
            "\n",
            "Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.\n",
            "\n",
            "EDUCATION BACKGROUND   \n",
            " \n",
            "12/2013 – 11/2017 Technological University (Dawei)\n",
            "\n",
            "in Electronics.  Interested\n",
            "\n",
            "Question: <pad> What  are  your  thoughts  on  the  potential  negative  impacts  of  technology  on  society?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What are your core beliefs regarding the role of technology in shaping society?': {'question': 'What are your core beliefs regarding the role of technology in shaping society?', 'chat_history': [HumanMessage(content='How many years of work experience do you have?'), AIMessage(content='<pad> Phue  Pwint  Thwe  has  a  professional  experience  in  Fiber-To-The-Home  (FTTH)  Network  Professional  Experience,  but  the  information  provided  does  not  specify  how  many  years  of  work  experience  she  has.\\n'), HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='<pad> Phue  Pwint  Thwe  has  been  involved  in  the  IT  field,  specifically  in  financial  career  and  personal  development.\\n'), HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  currently  a  Customer  Service  Engineer  (FTTH)  with  the  postgraduate  research  opportunity  at  AIT  University.  In  this  role,  she  provides  technical  support  and  troubleshooting  to  customers  using  fiber-optic  technology.  She  also  prepares  daily,  weekly,  and  monthly  reports  for  her  related  team.\\n')], 'answer': '<pad> As  an  AI  assistant,  I  do  not  have  personal  opinions  or  beliefs.  However,  based  on  my  training  data,  it  is  clear  that  technology  has  the  potential  to  negatively  impact  society  in  various  ways,  including:\\n 1.  Cybersecurity:  Technology  can  be  used  for  surveillance  and  hacking,  which  can  lead  to  increased  cybercrime  and  harm  to  individuals  and  communities.\\n 2.  Environmental  degradation:  The  use  of  electronic  devices  such  as  smartphones  and  other  gadgets  can  contribute  to  environmental  destruction  and  pollution.\\n 3.  Social  isolation:  Technological  advancements  can  create  social  isolation  among  people,  leading  to  feelings  of  loneliness  and  depression.\\n 4.  Economic  inequality:  Inequality  in  access  to  resources  and  opportunities  can  result  in  economic  instability  and  poverty.\\n 5.  Mental  health ', 'source_documents': [Document(page_content='Bachelor of Technology (Electronics).  \\n \\n12/2017 – 11/2019 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='EDUCATION BACKGROUND   \\n \\n12/2013 – 11/2017 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='in Electronics.  Interested', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What type of work or industry have you been involved in?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  has  been  involved  in  the  IT  field,  specifically  in  financial  career  and  personal  development.\n",
            "\n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  currently  a  Customer  Service  Engineer  (FTTH)  with  the  postgraduate  research  opportunity  at  AIT  University.  In  this  role,  she  provides  technical  support  and  troubleshooting  to  customers  using  fiber-optic  technology.  She  also  prepares  daily,  weekly,  and  monthly  reports  for  her  related  team.\n",
            "\n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: <pad> As  an  AI  assistant,  I  do  not  have  personal  opinions  or  beliefs.  However,  based  on  my  training  data,  it  is  clear  that  technology  has  the  potential  to  negatively  impact  society  in  various  ways,  including:\n",
            " 1.  Cybersecurity:  Technology  can  be  used  for  surveillance  and  hacking,  which  can  lead  to  increased  cybercrime  and  harm  to  individuals  and  communities.\n",
            " 2.  Environmental  degradation:  The  use  of  electronic  devices  such  as  smartphones  and  other  gadgets  can  contribute  to  environmental  destruction  and  pollution.\n",
            " 3.  Social  isolation:  Technological  advancements  can  create  social  isolation  among  people,  leading  to  feelings  of  loneliness  and  depression.\n",
            " 4.  Economic  inequality:  Inequality  in  access  to  resources  and  opportunities  can  result  in  economic  instability  and  poverty.\n",
            " 5.  Mental  health \n",
            "Follow Up Input: How do you think cultural values should influence technological advancements?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "Strong Multi-culture awareness\n",
            "\n",
            "EDUCATION BACKGROUND   \n",
            " \n",
            "12/2013 – 11/2017 Technological University (Dawei)\n",
            "\n",
            "Bachelor of Technology (Electronics).  \n",
            " \n",
            "12/2017 – 11/2019 Technological University (Dawei)\n",
            "\n",
            "Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community\n",
            "\n",
            "Question: <pad>  How  do  you  think  cultural  values  should  influence  technological  advancements?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'How do you think cultural values should influence technological advancements?': {'question': 'How do you think cultural values should influence technological advancements?', 'chat_history': [HumanMessage(content='What type of work or industry have you been involved in?'), AIMessage(content='<pad> Phue  Pwint  Thwe  has  been  involved  in  the  IT  field,  specifically  in  financial  career  and  personal  development.\\n'), HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  currently  a  Customer  Service  Engineer  (FTTH)  with  the  postgraduate  research  opportunity  at  AIT  University.  In  this  role,  she  provides  technical  support  and  troubleshooting  to  customers  using  fiber-optic  technology.  She  also  prepares  daily,  weekly,  and  monthly  reports  for  her  related  team.\\n'), HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content='<pad> As  an  AI  assistant,  I  do  not  have  personal  opinions  or  beliefs.  However,  based  on  my  training  data,  it  is  clear  that  technology  has  the  potential  to  negatively  impact  society  in  various  ways,  including:\\n 1.  Cybersecurity:  Technology  can  be  used  for  surveillance  and  hacking,  which  can  lead  to  increased  cybercrime  and  harm  to  individuals  and  communities.\\n 2.  Environmental  degradation:  The  use  of  electronic  devices  such  as  smartphones  and  other  gadgets  can  contribute  to  environmental  destruction  and  pollution.\\n 3.  Social  isolation:  Technological  advancements  can  create  social  isolation  among  people,  leading  to  feelings  of  loneliness  and  depression.\\n 4.  Economic  inequality:  Inequality  in  access  to  resources  and  opportunities  can  result  in  economic  instability  and  poverty.\\n 5.  Mental  health ')], 'answer': '<pad> Phue  Pwint  Thwe  believes  that  cultural  values  should  influence  technological  advancements  in  a  way  that  is  respectful  and  inclusive.  This  means  considering  the  needs,  beliefs,  and  perspectives  of  different  communities  when  designing  technologies  to  meet  their  unique  needs  and  goals.  For  example,  technology  can  be  designed  with  accessibility  in  mind,  or  it  can  prioritize  inclusivity  by  including  features  such  as  language  translation  tools  or  personalized  recommendations  for  users  based  on  their  preferences  and  experiences.\\n', 'source_documents': [Document(page_content='Strong Multi-culture awareness', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 1}), Document(page_content='EDUCATION BACKGROUND   \\n \\n12/2013 – 11/2017 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Bachelor of Technology (Electronics).  \\n \\n12/2017 – 11/2019 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Can you describe your current role or job responsibilities?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  currently  a  Customer  Service  Engineer  (FTTH)  with  the  postgraduate  research  opportunity  at  AIT  University.  In  this  role,  she  provides  technical  support  and  troubleshooting  to  customers  using  fiber-optic  technology.  She  also  prepares  daily,  weekly,  and  monthly  reports  for  her  related  team.\n",
            "\n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: <pad> As  an  AI  assistant,  I  do  not  have  personal  opinions  or  beliefs.  However,  based  on  my  training  data,  it  is  clear  that  technology  has  the  potential  to  negatively  impact  society  in  various  ways,  including:\n",
            " 1.  Cybersecurity:  Technology  can  be  used  for  surveillance  and  hacking,  which  can  lead  to  increased  cybercrime  and  harm  to  individuals  and  communities.\n",
            " 2.  Environmental  degradation:  The  use  of  electronic  devices  such  as  smartphones  and  other  gadgets  can  contribute  to  environmental  destruction  and  pollution.\n",
            " 3.  Social  isolation:  Technological  advancements  can  create  social  isolation  among  people,  leading  to  feelings  of  loneliness  and  depression.\n",
            " 4.  Economic  inequality:  Inequality  in  access  to  resources  and  opportunities  can  result  in  economic  instability  and  poverty.\n",
            " 5.  Mental  health \n",
            "Human: How do you think cultural values should influence technological advancements?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  believes  that  cultural  values  should  influence  technological  advancements  in  a  way  that  is  respectful  and  inclusive.  This  means  considering  the  needs,  beliefs,  and  perspectives  of  different  communities  when  designing  technologies  to  meet  their  unique  needs  and  goals.  For  example,  technology  can  be  designed  with  accessibility  in  mind,  or  it  can  prioritize  inclusivity  by  including  features  such  as  language  translation  tools  or  personalized  recommendations  for  users  based  on  their  preferences  and  experiences.\n",
            "\n",
            "Follow Up Input: As a master’s student, what is the most challenging aspect of your studies so far?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "the study period and initiate\n",
            "\n",
            "career and personal development.  Looking ahead to advance knowledge gain during the study period\n",
            "\n",
            "my advanced career opportunity with the postgraduate research at AIT University.\n",
            "\n",
            "Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community\n",
            "\n",
            "Question: <pad>  What  has  been  the  most  challenging  aspect  of  your  studies  so  far?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'As a master’s student, what is the most challenging aspect of your studies so far?': {'question': 'As a master’s student, what is the most challenging aspect of your studies so far?', 'chat_history': [HumanMessage(content='Can you describe your current role or job responsibilities?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  currently  a  Customer  Service  Engineer  (FTTH)  with  the  postgraduate  research  opportunity  at  AIT  University.  In  this  role,  she  provides  technical  support  and  troubleshooting  to  customers  using  fiber-optic  technology.  She  also  prepares  daily,  weekly,  and  monthly  reports  for  her  related  team.\\n'), HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content='<pad> As  an  AI  assistant,  I  do  not  have  personal  opinions  or  beliefs.  However,  based  on  my  training  data,  it  is  clear  that  technology  has  the  potential  to  negatively  impact  society  in  various  ways,  including:\\n 1.  Cybersecurity:  Technology  can  be  used  for  surveillance  and  hacking,  which  can  lead  to  increased  cybercrime  and  harm  to  individuals  and  communities.\\n 2.  Environmental  degradation:  The  use  of  electronic  devices  such  as  smartphones  and  other  gadgets  can  contribute  to  environmental  destruction  and  pollution.\\n 3.  Social  isolation:  Technological  advancements  can  create  social  isolation  among  people,  leading  to  feelings  of  loneliness  and  depression.\\n 4.  Economic  inequality:  Inequality  in  access  to  resources  and  opportunities  can  result  in  economic  instability  and  poverty.\\n 5.  Mental  health '), HumanMessage(content='How do you think cultural values should influence technological advancements?'), AIMessage(content='<pad> Phue  Pwint  Thwe  believes  that  cultural  values  should  influence  technological  advancements  in  a  way  that  is  respectful  and  inclusive.  This  means  considering  the  needs,  beliefs,  and  perspectives  of  different  communities  when  designing  technologies  to  meet  their  unique  needs  and  goals.  For  example,  technology  can  be  designed  with  accessibility  in  mind,  or  it  can  prioritize  inclusivity  by  including  features  such  as  language  translation  tools  or  personalized  recommendations  for  users  based  on  their  preferences  and  experiences.\\n')], 'answer': '<pad> Phue  Pwint  Thwe\\n', 'source_documents': [Document(page_content='the study period and initiate', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='career and personal development.  Looking ahead to advance knowledge gain during the study period', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='my advanced career opportunity with the postgraduate research at AIT University.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: What are your core beliefs regarding the role of technology in shaping society?\n",
            "Assistant: <pad> As  an  AI  assistant,  I  do  not  have  personal  opinions  or  beliefs.  However,  based  on  my  training  data,  it  is  clear  that  technology  has  the  potential  to  negatively  impact  society  in  various  ways,  including:\n",
            " 1.  Cybersecurity:  Technology  can  be  used  for  surveillance  and  hacking,  which  can  lead  to  increased  cybercrime  and  harm  to  individuals  and  communities.\n",
            " 2.  Environmental  degradation:  The  use  of  electronic  devices  such  as  smartphones  and  other  gadgets  can  contribute  to  environmental  destruction  and  pollution.\n",
            " 3.  Social  isolation:  Technological  advancements  can  create  social  isolation  among  people,  leading  to  feelings  of  loneliness  and  depression.\n",
            " 4.  Economic  inequality:  Inequality  in  access  to  resources  and  opportunities  can  result  in  economic  instability  and  poverty.\n",
            " 5.  Mental  health \n",
            "Human: How do you think cultural values should influence technological advancements?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  believes  that  cultural  values  should  influence  technological  advancements  in  a  way  that  is  respectful  and  inclusive.  This  means  considering  the  needs,  beliefs,  and  perspectives  of  different  communities  when  designing  technologies  to  meet  their  unique  needs  and  goals.  For  example,  technology  can  be  designed  with  accessibility  in  mind,  or  it  can  prioritize  inclusivity  by  including  features  such  as  language  translation  tools  or  personalized  recommendations  for  users  based  on  their  preferences  and  experiences.\n",
            "\n",
            "Human: As a master’s student, what is the most challenging aspect of your studies so far?\n",
            "Assistant: <pad> Phue  Pwint  Thwe\n",
            "\n",
            "Follow Up Input: What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "career and personal development.  Looking ahead to advance knowledge gain during the study period\n",
            "\n",
            "my advanced career opportunity with the postgraduate research at AIT University.\n",
            "\n",
            "the study period and initiate\n",
            "\n",
            "Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community\n",
            "\n",
            "Question: <pad>  What  are  your  long-term  academic  and  research  goals  as  a  master’s  student?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer for 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?': {'question': 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?', 'chat_history': [HumanMessage(content='What are your core beliefs regarding the role of technology in shaping society?'), AIMessage(content='<pad> As  an  AI  assistant,  I  do  not  have  personal  opinions  or  beliefs.  However,  based  on  my  training  data,  it  is  clear  that  technology  has  the  potential  to  negatively  impact  society  in  various  ways,  including:\\n 1.  Cybersecurity:  Technology  can  be  used  for  surveillance  and  hacking,  which  can  lead  to  increased  cybercrime  and  harm  to  individuals  and  communities.\\n 2.  Environmental  degradation:  The  use  of  electronic  devices  such  as  smartphones  and  other  gadgets  can  contribute  to  environmental  destruction  and  pollution.\\n 3.  Social  isolation:  Technological  advancements  can  create  social  isolation  among  people,  leading  to  feelings  of  loneliness  and  depression.\\n 4.  Economic  inequality:  Inequality  in  access  to  resources  and  opportunities  can  result  in  economic  instability  and  poverty.\\n 5.  Mental  health '), HumanMessage(content='How do you think cultural values should influence technological advancements?'), AIMessage(content='<pad> Phue  Pwint  Thwe  believes  that  cultural  values  should  influence  technological  advancements  in  a  way  that  is  respectful  and  inclusive.  This  means  considering  the  needs,  beliefs,  and  perspectives  of  different  communities  when  designing  technologies  to  meet  their  unique  needs  and  goals.  For  example,  technology  can  be  designed  with  accessibility  in  mind,  or  it  can  prioritize  inclusivity  by  including  features  such  as  language  translation  tools  or  personalized  recommendations  for  users  based  on  their  preferences  and  experiences.\\n'), HumanMessage(content='As a master’s student, what is the most challenging aspect of your studies so far?'), AIMessage(content='<pad> Phue  Pwint  Thwe\\n')], 'answer': '<pad> Phue  Pwint  Thwe\\n', 'source_documents': [Document(page_content='career and personal development.  Looking ahead to advance knowledge gain during the study period', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='my advanced career opportunity with the postgraduate research at AIT University.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='the study period and initiate', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Seeking to pursue opportunities to advance skills and deliver exemplary focus with a new community', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"How old are you?\",\n",
        "    \"What is your highest level of education?\",\n",
        "    \"What major or field of study did you pursue during your education?\",\n",
        "    \"How many years of work experience do you have?\",\n",
        "    \"What type of work or industry have you been involved in?\",\n",
        "    \"Can you describe your current role or job responsibilities?\",\n",
        "    \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
        "    \"How do you think cultural values should influence technological advancements?\",\n",
        "    \"As a master’s student, what is the most challenging aspect of your studies so far?\",\n",
        "    \"What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\"\n",
        "]\n",
        "\n",
        "answers = []\n",
        "for question in questions:\n",
        "    answer = Chain({\"question\": question})\n",
        "    answers.append(answer)\n",
        "    print(f\"Bot Answer for '{question}':\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBxd1R2eDV_H",
        "outputId": "d67a3957-da4c-4183-caae-9d0d00c3ec9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Who is Phue Pwint Thwe?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\n",
            "\n",
            "Follow Up Input: What is Phue Pwint Thwe's highest education?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "PHUE PWINT THWE\n",
            "\n",
            "(+95) 09422201502 * ppthwe99@gmail.com\n",
            "\n",
            "Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.\n",
            "\n",
            "EDUCATION BACKGROUND   \n",
            " \n",
            "12/2013 – 11/2017 Technological University (Dawei)\n",
            "\n",
            "Question: <pad> What  is  Phue  Pwint  Thwe's  highest  degree?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer: {'question': \"What is Phue Pwint Thwe's highest education?\", 'chat_history': [HumanMessage(content='Who is Phue Pwint Thwe?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\\n')], 'answer': \"<pad> Phue  Pwint  Thwe's  highest  degree  is  a  Bachelor  of  Engineering  from  Technological  University  (Dawei)  with  a  specialization  in  Electronics.\\n\", 'source_documents': [Document(page_content='PHUE PWINT THWE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='(+95) 09422201502 * ppthwe99@gmail.com', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='Bachelor of Engineering graduate from Technological University (Dawei) specialized in Electronics.', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='EDUCATION BACKGROUND   \\n \\n12/2013 – 11/2017 Technological University (Dawei)', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n"
          ]
        }
      ],
      "source": [
        "prompt_question = \"What is Phue Pwint Thwe's highest education?\"\n",
        "answer = Chain({\"question\": prompt_question})\n",
        "print(\"Bot Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vTJYUUADV0a",
        "outputId": "0405a46b-a77b-4094-a177-1adc6ef6f992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: Who is Phue Pwint Thwe?\n",
            "Assistant: <pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\n",
            "\n",
            "Human: What is Phue Pwint Thwe's highest education?\n",
            "Assistant: <pad> Phue  Pwint  Thwe's  highest  degree  is  a  Bachelor  of  Engineering  from  Technological  University  (Dawei)  with  a  specialization  in  Electronics.\n",
            "\n",
            "Follow Up Input: What are Phue Pwint Thwe's personal strength?\n",
            "Standalone question:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI am an AI assistant designed to provide general information about Phue Pwint Thwe.\n",
            "Please ask your question respectfully, and I will respond based on available documents.\n",
            "\n",
            "Context:\n",
            "PHUE PWINT THWE\n",
            "\n",
            "PERSONAL STRENGTHS \n",
            " \n",
            " Time management  \n",
            " Fast learner and physically fit\n",
            "\n",
            "(+95) 09422201502 * ppthwe99@gmail.com\n",
            "\n",
            "PERSONAL PROFILE\n",
            "\n",
            "Question: <pad> What  are  Phue  Pwint  Thwe's  personal  strengths?\n",
            "\n",
            "\n",
            "Gentle & Informative Answer:\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Bot Answer: {'question': \"What are Phue Pwint Thwe's personal strength?\", 'chat_history': [HumanMessage(content='Who is Phue Pwint Thwe?'), AIMessage(content='<pad> Phue  Pwint  Thwe  is  a  Bachelor  of  Engineering  graduate  from  Technological  University  (Dawei)  with  specialized  knowledge  in  Electronics.\\n'), HumanMessage(content=\"What is Phue Pwint Thwe's highest education?\"), AIMessage(content=\"<pad> Phue  Pwint  Thwe's  highest  degree  is  a  Bachelor  of  Engineering  from  Technological  University  (Dawei)  with  a  specialization  in  Electronics.\\n\")], 'answer': \"<pad> Phue  Pwint  Thwe's  personal  strengths  are:\\n *  Time  management\\n *  Fast  learner  and  physically  fit.\\n\", 'source_documents': [Document(page_content='PHUE PWINT THWE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='PERSONAL STRENGTHS \\n \\n Time management  \\n Fast learner and physically fit', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 1}), Document(page_content='(+95) 09422201502 * ppthwe99@gmail.com', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0}), Document(page_content='PERSONAL PROFILE', metadata={'source': '/content/drive/MyDrive/PHUE PWINT THWE Academic CV.pdf', 'page': 0})]}\n"
          ]
        }
      ],
      "source": [
        "prompt_question = \"What are Phue Pwint Thwe's personal strength?\"\n",
        "answer = Chain({\"question\": prompt_question})\n",
        "print(\"Bot Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "850pam2feFS8",
        "outputId": "55d04139-0f96-4291-cda4-ffc97a2ed60f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Phue  Pwint  Thwe's  education  is  a  Bachelor  of  Engineering  degree  from  Technological  University  (Dawei)  with  a  specialization  in  Electronics.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to generate responses\n",
        "def generate_response(question):\n",
        "    input_document = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in input_document])\n",
        "\n",
        "    prompt = PROMPT.format(context=context, question=question)\n",
        "    response = pipe(prompt, max_new_tokens=150)\n",
        "\n",
        "    return response[0][\"generated_text\"]\n",
        "\n",
        "# Test response\n",
        "print(generate_response(\"What is Phue Pwint Thwe's education?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRp-DyrcjwSb",
        "outputId": "a66ce5a6-1c1d-4849-eefb-6362ac84d32f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Phue  Pwint  Thwe's  native  language  is  Burmese.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test response\n",
        "print(generate_response(\"What is Phue Pwint Thwe's native language?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHVvrrQ9j3fv",
        "outputId": "c675dd0e-2813-4c28-d58c-218fd840b24c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Phue  Pwint  Thwe's  skills  include:\n",
            " *  Time  management\n",
            " *  Fast  learner  and  physically  fit\n",
            " *  Active  as  well  as  an  independent  team  player\n",
            " *  Careful  and  Meticulous.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test response\n",
        "print(generate_response(\"What are Phue Pwint Thwe's skills?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGuJh67JkmDw",
        "outputId": "495877de-ac03-4b29-8581-2733b72c164f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available OpenAI Models:\n",
            "gpt-4o-mini-audio-preview-2024-12-17\n",
            "dall-e-3\n",
            "dall-e-2\n",
            "gpt-4o-audio-preview-2024-10-01\n",
            "gpt-4o-audio-preview\n",
            "o1-mini-2024-09-12\n",
            "o1-mini\n",
            "omni-moderation-latest\n",
            "gpt-4o-mini-audio-preview\n",
            "omni-moderation-2024-09-26\n",
            "babbage-002\n",
            "tts-1-hd-1106\n",
            "text-embedding-3-large\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o-2024-11-20\n",
            "gpt-4o-2024-05-13\n",
            "tts-1-hd\n",
            "o1-preview\n",
            "o1-preview-2024-09-12\n",
            "gpt-4o-mini\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "gpt-4o-mini-search-preview\n",
            "tts-1-1106\n",
            "davinci-002\n",
            "gpt-3.5-turbo-1106\n",
            "gpt-4o-search-preview\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-4o-mini-search-preview-2025-03-11\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-4o-2024-08-06\n",
            "gpt-3.5-turbo\n",
            "gpt-3.5-turbo-16k\n",
            "gpt-4o\n",
            "text-embedding-3-small\n",
            "text-embedding-ada-002\n",
            "gpt-4.5-preview\n",
            "gpt-4.5-preview-2025-02-27\n",
            "gpt-4o-search-preview-2025-03-11\n",
            "tts-1\n",
            "whisper-1\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"Your-Key\"\n",
        "\n",
        "# Create OpenAI client with API key\n",
        "client = openai.OpenAI(api_key=openai.api_key)\n",
        "\n",
        "# Fetch available models\n",
        "response = client.models.list()\n",
        "\n",
        "print(\"Available OpenAI Models:\")\n",
        "for model in response.data:\n",
        "    print(model.id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "taQl6BGCkFPY",
        "outputId": "1a36e68b-5181-405f-82d5-fcf95de649e2"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-be721207a656>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenai_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Where did Phue Pwint Thwe study?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-be721207a656>\u001b[0m in \u001b[0;36mopenai_response\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPROMPT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopenai_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Test the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         | `astream`                 | '''                                                            | AsyncIterator[BaseMessageChunk]                                     | Defaults to yielding output of ainvoke.                                                          |\n\u001b[1;32m    157\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34...\n\u001b[0;32m--> 158\u001b[0;31m         | `astream_events`          | '''                                                            | AsyncIterator[StreamEvent]                                          | Event types: 'on_chat_model_start', 'on_chat_model_stream', 'on_chat_model_end'.                 |\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[...\n\u001b[1;32m    160\u001b[0m         | `batch`                   | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running invoke in concurrent threads.                                                |\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     def _get_ls_params(\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     ) -> LangSmithParams:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             )\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mgeneration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatGenerationChunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0minheritable_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             )\n\u001b[1;32m    413\u001b[0m             (run_manager,) = callback_manager.on_chat_model_start(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;36m2.\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mmore\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mjust\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;36m3.\u001b[0m \u001b[0mare\u001b[0m \u001b[0mbuilding\u001b[0m \u001b[0mchains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0magnostic\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0mtype\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0mvs\u001b[0m \u001b[0mchat\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         }\n\u001b[0;32m--> 441\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# Set API Key (Replace 'your-api-key' with your actual OpenAI API key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Your-key\"\n",
        "\n",
        "# Load OpenAI GPT Model\n",
        "openai_model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.7, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "def openai_response(question):\n",
        "    input_document = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in input_document])\n",
        "\n",
        "    prompt = PROMPT.format(context=context, question=question)\n",
        "    return openai_model.invoke(prompt)\n",
        "\n",
        "# Test the model\n",
        "print(openai_response(\"Where did Phue Pwint Thwe study?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C2exr9Yq5yA"
      },
      "outputs": [],
      "source": [
        "# Set Groq API Key (replace with key)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"Your-key\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wivWPCovq3Hm",
        "outputId": "838526ba-d6f2-41dd-c269-1d991d701e8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('data', [Model(id='llama-3.3-70b-specdec', created=1733505017, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=32768, public_apps=None), Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None), Model(id='qwen-2.5-coder-32b', created=1739494572, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-1b-preview', created=1727224268, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='allam-2-7b', created=1737672203, object='model', owned_by='SDAIA', active=True, context_window=4096, public_apps=None), Model(id='llama-3.2-11b-vision-preview', created=1727226869, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-3b-preview', created=1727224290, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='mistral-saba-24b', created=1739996492, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='llama-3.2-90b-vision-preview', created=1727226914, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='mixtral-8x7b-32768', created=1693721698, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='qwen-qwq-32b', created=1741214760, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='qwen-2.5-32b', created=1738789898, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='deepseek-r1-distill-qwen-32b', created=1738891590, object='model', owned_by='DeepSeek / Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None)])\n",
            "('object', 'list')\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "models = client.models.list()\n",
        "\n",
        "for model in models:\n",
        "    print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "I2YFOhwqTYTv"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"How old are you?\",\n",
        "    \"What is your highest level of education?\",\n",
        "    \"What major or field of study did you pursue during your education?\",\n",
        "    \"How many years of work experience do you have?\",\n",
        "    \"What type of work or industry have you been involved in?\",\n",
        "    \"Can you describe your current role or job responsibilities?\",\n",
        "    \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
        "    \"How do you think cultural values should influence technological advancements?\",\n",
        "    \"As a master’s student, what is the most challenging aspect of your studies so far?\",\n",
        "    \"What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "KKk4yo8mnril"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "\n",
        "\n",
        "# Load Groq Model (Use `llama3-70b` or `mixtral-8x7b`)\n",
        "groq_model = ChatGroq(model_name=\"allam-2-7b\", temperature=0.7)\n",
        "\n",
        "def groq_response(question):\n",
        "    input_document = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in input_document])\n",
        "\n",
        "    prompt = PROMPT.format(context=context, question=question)\n",
        "    return groq_model.invoke(prompt)\n",
        "\n",
        "# # Test the model\n",
        "# print(groq_response(\"Where did Phue Pwint Thwe study?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzkYjcmhT9LU",
        "outputId": "d77e5455-b963-4b6d-970a-6839af6f7bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot Answer for 'How old are you?': content='Based on the provided context, I cannot determine the age of Phue Pwint Thwe as the given information only includes their contact details and description of their personal profile, including their preference to be called \"Gentle & Informative.\" To answer your question accurately, I would need additional information about Phue Pwint Thwe\\'s date of birth or age. ' response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 109, 'total_tokens': 188, 'completion_time': 0.055875484, 'prompt_time': 0.00363542, 'queue_time': 0.233979595, 'total_time': 0.059510904}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-c1b329ca-1242-451a-8808-e26209181485-0'\n",
            "Bot Answer for 'What is your highest level of education?': content='Based on the provided context, your highest level of education is a Bachelor of Engineering degree, which you obtained from Technological University (Dawei) in 2017. You specialized in Electronics, indicating a focus on that particular field within your engineering studies. Your pursuit of a postgraduate research opportunity at AIT University further demonstrates your commitment to career and personal development, aiming to advance your knowledge gain during the study period. ' response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 139, 'total_tokens': 229, 'completion_time': 0.063573706, 'prompt_time': 0.004428177, 'queue_time': 0.23404999699999998, 'total_time': 0.068001883}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-d0e36009-d00b-432e-91aa-8ad9da320b6b-0'\n",
            "Bot Answer for 'What major or field of study did you pursue during your education?': content='Based on the provided context, during your education, you pursued the major of Bachelor of Engineering with a specialization in Electronics. This specialization is from Technological University (Dawei). ' response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 118, 'total_tokens': 159, 'completion_time': 0.029016188, 'prompt_time': 0.003769167, 'queue_time': 0.214555659, 'total_time': 0.032785355}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-060745c4-bdcb-4e69-b70c-fdd8da6b0ee7-0'\n",
            "Bot Answer for 'How many years of work experience do you have?': content='Based on the provided context, there is not enough information to accurately determine the exact number of years of work experience you have. However, we can infer that you have gained significant experience in the IT field of financial services and have expertise in Data Science, Artificial Intelligence Systems, and EnGenius Certified Network Specialist skills. Your focus on career and personal development, as well as your aspiration to advance knowledge during your study period in Fiber-to-the-Home (FTTH) network, indicates a diverse set of experiences and ongoing commitment to learning and growth. ' response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 135, 'total_tokens': 252, 'completion_time': 0.083689999, 'prompt_time': 0.004281105, 'queue_time': 0.471049924, 'total_time': 0.087971104}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-ca771aa8-bf57-488a-b624-00b9d86dac87-0'\n",
            "Bot Answer for 'What type of work or industry have you been involved in?': content='Based on the provided context, it seems that your work experience has primarily focused on Data Science, Artificial Intelligence, and IT in the field of financial services. You have also been involved in the development and deployment of Fiber-to-the-Home (FTTH) networks. Your career trajectory includes pursuing postgraduate research at AIT University, which aligns with your aspiration for career and personal development, as well as advancing your knowledge during the study period. ' response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 137, 'total_tokens': 233, 'completion_time': 0.068079271, 'prompt_time': 0.004275149, 'queue_time': 0.234015006, 'total_time': 0.07235442}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-f751506c-d5a3-4a3b-b381-76dfdf6ed928-0'\n",
            "Bot Answer for 'Can you describe your current role or job responsibilities?': content='Based on the provided context, it appears that your current role or job responsibilities primarily involve preparing various types of reports. In specific, you are responsible for creating daily, weekly, and monthly reports that are shared with the related team. Your focus seems to be on showcasing your expertise in this field while seeking opportunities to further develop your skills and deliver exceptional results. Recently, you have expressed interest in pursuing postgraduate research at AIT University, indicating a desire to advance your career and knowledge within this field. ' response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 125, 'total_tokens': 230, 'completion_time': 0.074144041, 'prompt_time': 0.003925085, 'queue_time': 0.23585371, 'total_time': 0.078069126}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-3096cbf5-281d-467d-bf39-15ac5a1ed78a-0'\n",
            "Bot Answer for 'What are your core beliefs regarding the role of technology in shaping society?': content='Based on the provided context, it seems that you hold a strong belief in the importance of technology in shaping society, considering your educational background in Electronics from Technological University (Dawei). Your ambition to pursue opportunities for skill advancement and join a new community demonstrates your commitment to the role of technology in society. \\n\\nIn terms of core beliefs, it can be inferred that you likely believe technology has the potential to improve and advance human lives, solve societal challenges, and drive economic growth. You may also value the ethical responsibility tied to technological advancements, ensuring that they are developed responsibly to minimize negative impacts and maximize positive change. \\n\\nHowever, it is essential to mention that your personal beliefs on this topic might evolve over time as you gain more experiences and exposure to diverse perspectives. ' response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 169, 'total_tokens': 336, 'completion_time': 0.117983524, 'prompt_time': 0.005202764, 'queue_time': 0.214435542, 'total_time': 0.123186288}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-c9ef7227-8588-4339-b87e-46f8de58ef15-0'\n",
            "Bot Answer for 'How do you think cultural values should influence technological advancements?': content='I believe that cultural values play a significant role in shaping and guiding technological advancements. When developing technologies, it is essential to consider and respect various cultural perspectives to ensure that innovations are inclusive, respectful, and beneficial for all stakeholders. Here are some ways in which cultural values can influence technological advancements:\\n\\n1. Ethical Considerations: Cultural values often shape our perceptions of what is right, wrong, and acceptable. Incorporating these values into technology design can help ensure that innovations are developed in a responsible manner, respecting the privacy, autonomy, and dignity of individuals.\\n\\n2. User Experience: Understanding cultural values can help developers tailor technology to meet the unique needs and preferences of different communities. This can result in more user-friendly interfaces, more intuitive systems, and better acceptance of new technologies.\\n\\n3. Cultural Adaptation: Technology should not be seen as a homogenizing force that alters or erodes cultural heritage. Instead, it should be used to support and preserve diverse cultural practices, traditions, and beliefs. This can be achieved by incorporating cultural insights into technology design and development.\\n\\n4. Accessibility: Different cultures have varying levels of access to resources, technology infrastructure, and expertise. By taking these factors into account, technology can be designed to minimize digital divides and ensure that innovations benefit all members of society, irrespective of their cultural background.\\n\\n5. Social Impact: Technology can have significant societal impacts, both positive and negative. Understanding and respecting cultural values can help developers ensure that innovations do not unintentionally reinforce existing social inequalities or create new ones.\\n\\nIn conclusion, cultural values should significantly influence technological advancements to ensure that innovations are developed in a way that respects, embraces, and enhances the richness and diversity of human cultures, while also addressing the social, economic, and environmental challenges of our time. ' response_metadata={'token_usage': {'completion_tokens': 398, 'prompt_tokens': 156, 'total_tokens': 554, 'completion_time': 0.280549618, 'prompt_time': 0.004743571, 'queue_time': 0.214648395, 'total_time': 0.285293189}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-9709a818-b495-4dc9-b5bb-bf20590808b1-0'\n",
            "Bot Answer for 'As a master’s student, what is the most challenging aspect of your studies so far?': content=\"As a master's student, focusing on your career and personal development during the study period can present several challenges. Among these challenges, the most daunting aspect might be juggling your academic pursuits with the aim of advancing knowledge and gaining new skills. This requires diligent time management, prioritization, and a strong commitment to your goals.\\n\\nAnother challenging aspect involves adapting to the demands of postgraduate research at AIT University. The advanced level of study may require you to grapple with complex concepts, conduct extensive research, and engage in critical thinking, which can be demanding both intellectually and emotionally.\\n\\nFurthermore, seeking opportunities to advance your skills and deliver exemplary focus within a new community can introduce additional challenges. Adapting to the academic culture, establishing meaningful connections, and finding motivation in a different environment can be obstacles to overcome.\\n\\nOverall, these challenges are part of the growth process as a master's student, and by remaining focused, dedicated, and seeking support when needed, you can overcome these obstacles and successfully progress in your career and personal development. \" response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 119, 'total_tokens': 348, 'completion_time': 0.163521648, 'prompt_time': 0.00379561, 'queue_time': 0.236089214, 'total_time': 0.167317258}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-6e9f5abc-bad4-4a6e-bee5-ed6b4108e963-0'\n",
            "Bot Answer for 'What specific research interests or academic goals do you hope to achieve during your time as a master’s student?': content=\"Based on your expressed interests in career and personal development, as well as advancing knowledge during your study period, it seems that your primary academic goals focus on career advancement and skill development. Specifically, pursuing a postgraduate research opportunity at AIT University aligns with these goals.\\n\\nThrough your master's studies, you aim to gain advanced knowledge and expertise in your field, which will enhance your career prospects and qualifications. Additionally, you desire to deliver exemplary focus and engage with a new community, indicating a desire to contribute and learn from diverse perspectives.\\n\\nTo achieve these goals, you plan to actively engage with your coursework, attend workshops or seminars, and possibly engage in research projects or internships related to your chosen field. You aim to broaden your skillset and gain practical experience, preparing you for an advanced career opportunity upon graduation. By the end of your master's program, you aspire to have made significant strides in your academic journey and be well-equipped to tackle challenges in your career after graduation. \" response_metadata={'token_usage': {'completion_tokens': 218, 'prompt_tokens': 122, 'total_tokens': 340, 'completion_time': 0.154191043, 'prompt_time': 0.003966154, 'queue_time': 0.233921531, 'total_time': 0.158157197}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_1612a676d6', 'finish_reason': 'stop', 'logprobs': None} id='run-49f6a151-30f1-4e4f-92c8-f24387087578-0'\n"
          ]
        }
      ],
      "source": [
        "# Get responses for each question\n",
        "answers = []\n",
        "for question in questions:\n",
        "    response = groq_response(question)\n",
        "    answers.append(response)\n",
        "    print(f\"Bot Answer for '{question}':\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpv0zXoGnwCI",
        "outputId": "886996ca-e6dd-46e8-df3d-f4d22ce9ca50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Phue Pwint Thwe's native language is Burmese. \\n\\nNote: As an AI, I can only provide information based on the data available to me. Phue Pwint Thwe's background information has been included for your understanding. If you have further questions or need assistance on a different topic, feel free to ask! \" response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 179, 'total_tokens': 255, 'completion_time': 0.053556563, 'prompt_time': 0.00542862, 'queue_time': 0.480825603, 'total_time': 0.058985183}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-2d325fe6-2f36-4534-b33c-5073e4c910b8-0'\n"
          ]
        }
      ],
      "source": [
        "print(groq_response(\"What is Phue Pwint Thwe native language?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK6TGwdbplJM",
        "outputId": "bd9fad79-631c-4b0a-958e-3913ca0b7af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"To determine Phue Pwint Thwe's age, I would need to know the year she completed her education. Based on the information provided, we only know her education background and contact details. Please provide the completion year of her Technological University (Dawei) study, and I will calculate her age for you. \" response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 149, 'total_tokens': 217, 'completion_time': 0.047934634, 'prompt_time': 0.004633799, 'queue_time': 0.23812165000000002, 'total_time': 0.052568433}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-7c454e40-7778-48d4-b6e0-e673c086e030-0'\n"
          ]
        }
      ],
      "source": [
        "print(groq_response(\"How old is she?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk7ea3uN93Te",
        "outputId": "cdf8f2d8-05a5-4a2e-cace-dc932569d8b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Based on the provided information, Phue Pwint Thwe seems to possess several strengths. These strengths include:\\n\\n1. Time Management: This demonstrates her ability to prioritize tasks, organize her time effectively, and meet deadlines.\\n2. Fast Learner and Physically Fit: It indicates her adaptability, quick comprehension, and good physical condition which contribute to her productivity and performance.\\n3. Active and Independent Team Player: Phue Pwint Thwe shows her ability to work well both independently and as part of a team, contributing positively to group projects.\\n4. Careful and Meticulous: These qualities showcase her attention to detail, diligence, and commitment to accuracy in her work.\\n\\nYou may contact Phue Pwint Thwe using her contact details: (+95) 09422201502 (phone number) and ppthwe99@gmail.com (email address). ' response_metadata={'token_usage': {'completion_tokens': 203, 'prompt_tokens': 149, 'total_tokens': 352, 'completion_time': 0.142806465, 'prompt_time': 0.004624609, 'queue_time': 0.463988188, 'total_time': 0.147431074}, 'model_name': 'allam-2-7b', 'system_fingerprint': 'fp_77a9c76dc9', 'finish_reason': 'stop', 'logprobs': None} id='run-dba10051-0e02-4b0c-8da2-c5fa09a9d66d-0'\n"
          ]
        }
      ],
      "source": [
        "print(groq_response(\"what are her strength?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJVwkNrlGjCw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01a0fa8a4c9849378ca36d86502ae286": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03187ace2ae14476b6c33103a0b7ce52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0ade36a76964fa1aa160377706a861a",
            "placeholder": "​",
            "style": "IPY_MODEL_5cfd19672d8546a2b4df2114651f2d3c",
            "value": " 1.48k/1.48k [00:00&lt;00:00, 164kB/s]"
          }
        },
        "057931756a574bb6b8fdf83e9f318b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ae1ff3857694a0196319d2868da5570",
            "max": 2360171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1eb927dcb0434deaa06071830334b438",
            "value": 2360171
          }
        },
        "05aa013bc1b4459bb88e677c6a5eb2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "061091dab5c044e3a7f262c06356e720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ae93f56ecf142c2b28f5e2b4d2aee58",
            "placeholder": "​",
            "style": "IPY_MODEL_05aa013bc1b4459bb88e677c6a5eb2e3",
            "value": "Downloading config.json: 100%"
          }
        },
        "083212ca1ccf4d18aaa41a49eae482dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92c22228eac94149ae883773ac2dcc67",
            "placeholder": "​",
            "style": "IPY_MODEL_7d4923f33e794ecaa228592631861305",
            "value": " 115/115 [00:00&lt;00:00, 14.3kB/s]"
          }
        },
        "0d36ead40d704b54a504db6688e7c364": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1056c4584ba44735bafa8029b905f5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120c64db30a34386926cceade05be14f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14702bce773c46a4b367de6c6b3f5139": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc165f194f7c4c4f9ac2f564b626ca9a",
              "IPY_MODEL_2711727bcdb747bcbbdf0a6b6579fcfa",
              "IPY_MODEL_bd66c79cf7044338b33c9b2f72c2b1dc"
            ],
            "layout": "IPY_MODEL_ca5ad3c29a204c178ff9fe77e3196834"
          }
        },
        "1802ae8619ff4f2abdb868934be4b357": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1dd05dae35494607a1e735bf4d6ed11e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eacc1b9159648529f36766bcfc927c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eb927dcb0434deaa06071830334b438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22e49617f43c432d856e2d42afcae380": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc4644495fa34a8ab0314ef91d7a1e30",
              "IPY_MODEL_9edc0f613dce43c694170d3a2d21ea3c",
              "IPY_MODEL_083212ca1ccf4d18aaa41a49eae482dc"
            ],
            "layout": "IPY_MODEL_aba0492d46d340e3a23f800b776d12ee"
          }
        },
        "23e067a378164f60b753ce6f1710e3ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "243f7cd7f50d472bbe8ec75d6d02f0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2599e54b10a44a5cb627f8e58aa4e901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "266cf3c0ed444c4493eba2054469c74e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2711727bcdb747bcbbdf0a6b6579fcfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1b8fcebbcd94881b80b45ca0afba327",
            "max": 438546812,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d976dbd11f5e4e4d8ff4f9b9b07fd2af",
            "value": 438546812
          }
        },
        "271f8a1080184ca5988b6ec45e9cce0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82a2d495b5b5458bb260a514bf07fb43",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc5b1785c3b84b5380bf628201b7c645",
            "value": 122
          }
        },
        "2a3c9352d9c04817bcf7089fa67ba34f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2baaa45beed14fe18057fa2b83d4df7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c59bcb246540348deb876e8c4127e2",
            "placeholder": "​",
            "style": "IPY_MODEL_0d36ead40d704b54a504db6688e7c364",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "2d7b9fc5ebc445fc99186d0673af8b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e53e3c07858449da0187fd05d859017": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9d156e7cff4d3dbbad44a67d568c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dc139e4670b4d79a60bdc71dcbcbbe4",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d657c1894e4f47508318add12663ebb6",
            "value": 791656
          }
        },
        "312ed65f33434b11b126ef21ecf3132b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321da4eadbb0460590ecc6f1fa7ff6a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c59bcb246540348deb876e8c4127e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3440fb6b29ea4dd796bcfcf749f156c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "366afa6daa0244d2a9ab60fa98e1da76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c653b9a965423cbb7828aa43baf75e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c1088214f954616926a66e2b0272cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_312ed65f33434b11b126ef21ecf3132b",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf493dbdddd4f63b40ab87cedf4bc76",
            "value": "Downloading .gitattributes: 100%"
          }
        },
        "3dc139e4670b4d79a60bdc71dcbcbbe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e3e2f74735848a4b91e84d5b97c690f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43c70c25fcaa447cbb419583ef5c0d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46ecf6d5b2364948857e408e952dc877": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47cfd264cc5a4f70a9852e7fa0a5e6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47ee8e0f5dc3484e979d613f33256615": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4828a17c698849dcbf25291c4825624d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49fae5bfc0704f6eb74aab20735c2c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae1ff3857694a0196319d2868da5570": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51cd996d1d6c4a078813c264ca243c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e808292ac44d0bb5804480c2f68070",
            "max": 270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37c653b9a965423cbb7828aa43baf75e",
            "value": 270
          }
        },
        "53b88c5dc76c4c42b18d9f7d935cf655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "554cfabb1dea49c4bb10d27d76aa6c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6630f4fa37e3488c858b4a6ae0623893",
            "placeholder": "​",
            "style": "IPY_MODEL_f83295ee00ca45bf9469de5cffd080ec",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "556b19bec5224a868f332beb71f5cfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5610ac85e2cf45dc84b1978cf2318898": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6172699e6f784e6f9b5a7ef071df159f",
              "IPY_MODEL_51cd996d1d6c4a078813c264ca243c97",
              "IPY_MODEL_a335fa3fd6b2452780a92a2aa7cb52e4"
            ],
            "layout": "IPY_MODEL_cc76b2e3cdf744f9a4345b3d6242004c"
          }
        },
        "58fceb527e7143bc967fe426d0f91bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c8dbf745dc64d35a844e48bd1b8dd8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cfd19672d8546a2b4df2114651f2d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "606562365d6d47118a9fbe26be605539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6122dccb0ca84103861c416b1a4fcdf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6129f2bc7f2349d7930a5c86f21aa938": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6172699e6f784e6f9b5a7ef071df159f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2a7f7693544e58b2f32ffae1fd57e6",
            "placeholder": "​",
            "style": "IPY_MODEL_fefc116b78804bf8a77c586cf9fce6d8",
            "value": "Downloading config.json: 100%"
          }
        },
        "63110cba947d48b9931dbf03fe0cb354": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64cfe274e3274f89ba5aa78540bb86af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fada8250faf946d2a527de651a16f304",
            "max": 66227,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3440fb6b29ea4dd796bcfcf749f156c5",
            "value": 66227
          }
        },
        "6630f4fa37e3488c858b4a6ae0623893": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3475fe748c4003a401ca0f6e2baf3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bf95547aa314324b16acd01c4d63ad0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ebafc8e3bf4427ab8ba09b6d0742843": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706e124f80674223945d6324d281b6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2599e54b10a44a5cb627f8e58aa4e901",
            "placeholder": "​",
            "style": "IPY_MODEL_ca92559259b641b5915e6ee29b8760e5",
            "value": " 461/461 [00:00&lt;00:00, 60.4kB/s]"
          }
        },
        "716f2db1a197491ab8ecfd17f2d276ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_120c64db30a34386926cceade05be14f",
            "placeholder": "​",
            "style": "IPY_MODEL_a978989f91fb49789d9e93facb381727",
            "value": " 66.2k/66.2k [00:00&lt;00:00, 8.25MB/s]"
          }
        },
        "7ae93f56ecf142c2b28f5e2b4d2aee58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d0057b98f044605b26a70a59a80a0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d4923f33e794ecaa228592631861305": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ee814d0805046d6b7b9679b52386be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47ee8e0f5dc3484e979d613f33256615",
            "max": 2429,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_997240772c3c46e38cf92b1899dcf61c",
            "value": 2429
          }
        },
        "7f8f2f50c27b46a2904871d246d0e187": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a2d495b5b5458bb260a514bf07fb43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "873731292e6b40228764055852266c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d708b5c91b904459a37870128b88b33a",
              "IPY_MODEL_64cfe274e3274f89ba5aa78540bb86af",
              "IPY_MODEL_716f2db1a197491ab8ecfd17f2d276ee"
            ],
            "layout": "IPY_MODEL_d0727b41658e458da886122b6ff3c51a"
          }
        },
        "8a0da4680661447594226a85b33ba154": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e3be967292e4100b5ec53356c22c298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_366afa6daa0244d2a9ab60fa98e1da76",
            "placeholder": "​",
            "style": "IPY_MODEL_7d0057b98f044605b26a70a59a80a0e4",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "8fcba16f63af4b2995b44fef801e7fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c63afc7cf86d4d98a953ee3fdf0b369e",
              "IPY_MODEL_057931756a574bb6b8fdf83e9f318b04",
              "IPY_MODEL_d8def0f8dfa94378bb0f7a0ed547fd27"
            ],
            "layout": "IPY_MODEL_49fae5bfc0704f6eb74aab20735c2c3b"
          }
        },
        "91237860b841413298b654602f308298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9bb8b32b6184137a2e87e842e942240",
            "max": 2201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff0a780283644b0caab8f1040bef7a83",
            "value": 2201
          }
        },
        "912ec7d4aae543549e3fd69ec4bce140": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92c22228eac94149ae883773ac2dcc67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94fb85d5bb8f401a827b28733d2f30fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9736fc23961a4227b0ca20c5e68e6437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97d965131a0c447486af9d981e4b7b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "997240772c3c46e38cf92b1899dcf61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99a8cc9ed7db427dae73f535d9f73922": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cd2f470abe44e36b0bb596228ed73d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6122dccb0ca84103861c416b1a4fcdf3",
            "max": 1477,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abbaab5b3c064b89bb1187c7a2d1070f",
            "value": 1477
          }
        },
        "9edc0f613dce43c694170d3a2d21ea3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e067a378164f60b753ce6f1710e3ab",
            "max": 115,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53b88c5dc76c4c42b18d9f7d935cf655",
            "value": 115
          }
        },
        "9f716d5d3fc941218634ec9bb0384ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b07de924f11043bfbb4aaa8e8042cb99",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e287e3e392b14dc68d97ae1f8097431a",
            "value": 53
          }
        },
        "a0ade36a76964fa1aa160377706a861a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a11cf2bdf30a49cbbfa11a386461a407": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_554cfabb1dea49c4bb10d27d76aa6c93",
              "IPY_MODEL_7ee814d0805046d6b7b9679b52386be9",
              "IPY_MODEL_cee3785ab48f47ee9e9952a6ce420d0c"
            ],
            "layout": "IPY_MODEL_7f8f2f50c27b46a2904871d246d0e187"
          }
        },
        "a1be472f746b4274b6141707886349b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_321da4eadbb0460590ecc6f1fa7ff6a3",
            "max": 2422360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0d3539c0914464ea1097d4c2c3baa54",
            "value": 2422360
          }
        },
        "a335fa3fd6b2452780a92a2aa7cb52e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99a8cc9ed7db427dae73f535d9f73922",
            "placeholder": "​",
            "style": "IPY_MODEL_e7ba6ffa69a54f8cb26ab6aa55cc610a",
            "value": " 270/270 [00:00&lt;00:00, 38.0kB/s]"
          }
        },
        "a8113dfe64c848108a834dc60e188fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e3be967292e4100b5ec53356c22c298",
              "IPY_MODEL_a1be472f746b4274b6141707886349b2",
              "IPY_MODEL_cbee5d7444434b479f314ea534bf8ecf"
            ],
            "layout": "IPY_MODEL_f6f04e4da07f45d8b8847c703df4a5e6"
          }
        },
        "a978989f91fb49789d9e93facb381727": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa2e011799c94c0e9ceb33f496337499": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e53e3c07858449da0187fd05d859017",
            "max": 1550,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a3475fe748c4003a401ca0f6e2baf3c",
            "value": 1550
          }
        },
        "aa6a485cbee1487ab1f4bb6a5cb640eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba0492d46d340e3a23f800b776d12ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abbaab5b3c064b89bb1187c7a2d1070f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b07de924f11043bfbb4aaa8e8042cb99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20834e68b4e49578e0733e3f0a9b867": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_266cf3c0ed444c4493eba2054469c74e",
            "placeholder": "​",
            "style": "IPY_MODEL_912ec7d4aae543549e3fd69ec4bce140",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "b41ea69dc06a4b30bbbb4e4b76f0d0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b51c68a1e39a46b88fb4927457165a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bc10e431d345b1a23dc3e3b9c88d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b20834e68b4e49578e0733e3f0a9b867",
              "IPY_MODEL_9f716d5d3fc941218634ec9bb0384ca7",
              "IPY_MODEL_be26253cdd67426e93241ff7d172e157"
            ],
            "layout": "IPY_MODEL_1056c4584ba44735bafa8029b905f5f5"
          }
        },
        "b9c662c8eeaf44928974542b501c7401": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a0fa8a4c9849378ca36d86502ae286",
            "placeholder": "​",
            "style": "IPY_MODEL_6129f2bc7f2349d7930a5c86f21aa938",
            "value": "Downloading spiece.model: 100%"
          }
        },
        "bc4644495fa34a8ab0314ef91d7a1e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9868ce97ad4ddb9eae445e36149066",
            "placeholder": "​",
            "style": "IPY_MODEL_46ecf6d5b2364948857e408e952dc877",
            "value": "Downloading config.json: 100%"
          }
        },
        "bc90b619bc154ca2b4e43df89dfd369a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd66c79cf7044338b33c9b2f72c2b1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5934b2790694ca9882792785006f4b9",
            "placeholder": "​",
            "style": "IPY_MODEL_8a0da4680661447594226a85b33ba154",
            "value": " 439M/439M [00:01&lt;00:00, 236MB/s]"
          }
        },
        "be26253cdd67426e93241ff7d172e157": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63110cba947d48b9931dbf03fe0cb354",
            "placeholder": "​",
            "style": "IPY_MODEL_606562365d6d47118a9fbe26be605539",
            "value": " 53.0/53.0 [00:00&lt;00:00, 7.43kB/s]"
          }
        },
        "be35c939829242c7bf00e46e4a38bdd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9c662c8eeaf44928974542b501c7401",
              "IPY_MODEL_2f9d156e7cff4d3dbbad44a67d568c70",
              "IPY_MODEL_c42a5adbae174eb9b605cbe3d3acc0c2"
            ],
            "layout": "IPY_MODEL_6ebafc8e3bf4427ab8ba09b6d0742843"
          }
        },
        "c42a5adbae174eb9b605cbe3d3acc0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe00ce45ecaf414d8945db61461d8846",
            "placeholder": "​",
            "style": "IPY_MODEL_5c8dbf745dc64d35a844e48bd1b8dd8b",
            "value": " 792k/792k [00:00&lt;00:00, 81.0MB/s]"
          }
        },
        "c48a11b8177c403eb61ec4d40c62c974": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c59ce74d714544308864857ee4343b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbd3c6c27a224085a214cdc5c8db0dd9",
            "max": 461,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1802ae8619ff4f2abdb868934be4b357",
            "value": 461
          }
        },
        "c63afc7cf86d4d98a953ee3fdf0b369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfdafcdf084f42b9bb3c02e41eeb51cf",
            "placeholder": "​",
            "style": "IPY_MODEL_cfda8d5b6426496ab6060a1f4db39af9",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "c7e808292ac44d0bb5804480c2f68070": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9a871dea8b0402f82ca5561e4af4647": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5ad3c29a204c178ff9fe77e3196834": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca92559259b641b5915e6ee29b8760e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbd3c6c27a224085a214cdc5c8db0dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbee5d7444434b479f314ea534bf8ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4828a17c698849dcbf25291c4825624d",
            "placeholder": "​",
            "style": "IPY_MODEL_3e3e2f74735848a4b91e84d5b97c690f",
            "value": " 2.42M/2.42M [00:00&lt;00:00, 10.9MB/s]"
          }
        },
        "cc165f194f7c4c4f9ac2f564b626ca9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6a6432491074767aae72160e7d39a1d",
            "placeholder": "​",
            "style": "IPY_MODEL_58fceb527e7143bc967fe426d0f91bf0",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "cc76b2e3cdf744f9a4345b3d6242004c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc93a5fff21a4d93a66422e58ee9fe28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a871dea8b0402f82ca5561e4af4647",
            "placeholder": "​",
            "style": "IPY_MODEL_43c70c25fcaa447cbb419583ef5c0d4c",
            "value": "Downloading modules.json: 100%"
          }
        },
        "cee3785ab48f47ee9e9952a6ce420d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bf95547aa314324b16acd01c4d63ad0",
            "placeholder": "​",
            "style": "IPY_MODEL_b41ea69dc06a4b30bbbb4e4b76f0d0e4",
            "value": " 2.43k/2.43k [00:00&lt;00:00, 314kB/s]"
          }
        },
        "cfda8d5b6426496ab6060a1f4db39af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0727b41658e458da886122b6ff3c51a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d657c1894e4f47508318add12663ebb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d708b5c91b904459a37870128b88b33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b51c68a1e39a46b88fb4927457165a1f",
            "placeholder": "​",
            "style": "IPY_MODEL_bc90b619bc154ca2b4e43df89dfd369a",
            "value": "Downloading README.md: 100%"
          }
        },
        "d8def0f8dfa94378bb0f7a0ed547fd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47cfd264cc5a4f70a9852e7fa0a5e6fe",
            "placeholder": "​",
            "style": "IPY_MODEL_1eacc1b9159648529f36766bcfc927c8",
            "value": " 2.36M/2.36M [00:00&lt;00:00, 42.3MB/s]"
          }
        },
        "d976dbd11f5e4e4d8ff4f9b9b07fd2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da2a7f7693544e58b2f32ffae1fd57e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf493dbdddd4f63b40ab87cedf4bc76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc5b1785c3b84b5380bf628201b7c645": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc997a8f380a490c87b501a5600c1d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd72728d7c034146b3d6f4909f7a0778": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df2ffbe7a57b431880dee906a70fa785": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa6a485cbee1487ab1f4bb6a5cb640eb",
            "placeholder": "​",
            "style": "IPY_MODEL_9736fc23961a4227b0ca20c5e68e6437",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "dfdafcdf084f42b9bb3c02e41eeb51cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d3539c0914464ea1097d4c2c3baa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e287e3e392b14dc68d97ae1f8097431a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3e9be76ab0e4f91a3ecc34f0f038631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df2ffbe7a57b431880dee906a70fa785",
              "IPY_MODEL_271f8a1080184ca5988b6ec45e9cce0c",
              "IPY_MODEL_e80eff8576c849df992ff15c4f4641f6"
            ],
            "layout": "IPY_MODEL_2d7b9fc5ebc445fc99186d0673af8b4f"
          }
        },
        "e50d12e74a3944c5a3276b427d8b9769": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e85dd7dda34943b6a0822de3f8646da5",
            "placeholder": "​",
            "style": "IPY_MODEL_97d965131a0c447486af9d981e4b7b76",
            "value": " 1.55k/1.55k [00:00&lt;00:00, 214kB/s]"
          }
        },
        "e5934b2790694ca9882792785006f4b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a6432491074767aae72160e7d39a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ba6ffa69a54f8cb26ab6aa55cc610a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e80eff8576c849df992ff15c4f4641f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a3c9352d9c04817bcf7089fa67ba34f",
            "placeholder": "​",
            "style": "IPY_MODEL_c48a11b8177c403eb61ec4d40c62c974",
            "value": " 122/122 [00:00&lt;00:00, 14.1kB/s]"
          }
        },
        "e85dd7dda34943b6a0822de3f8646da5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9bb8b32b6184137a2e87e842e942240": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee9868ce97ad4ddb9eae445e36149066": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b8fcebbcd94881b80b45ca0afba327": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3575f672da04e5196e36f0392167aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c1088214f954616926a66e2b0272cc0",
              "IPY_MODEL_9cd2f470abe44e36b0bb596228ed73d4",
              "IPY_MODEL_03187ace2ae14476b6c33103a0b7ce52"
            ],
            "layout": "IPY_MODEL_243f7cd7f50d472bbe8ec75d6d02f0b3"
          }
        },
        "f6f04e4da07f45d8b8847c703df4a5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83295ee00ca45bf9469de5cffd080ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f860fb31236c4d5ab571b2e5016ad68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc93a5fff21a4d93a66422e58ee9fe28",
              "IPY_MODEL_c59ce74d714544308864857ee4343b0b",
              "IPY_MODEL_706e124f80674223945d6324d281b6c6"
            ],
            "layout": "IPY_MODEL_1dd05dae35494607a1e735bf4d6ed11e"
          }
        },
        "f9655cb71af6478d82c35409e2bdd81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2baaa45beed14fe18057fa2b83d4df7e",
              "IPY_MODEL_91237860b841413298b654602f308298",
              "IPY_MODEL_ff4cf821963b4eb9a872eeffb4fb60e0"
            ],
            "layout": "IPY_MODEL_94fb85d5bb8f401a827b28733d2f30fe"
          }
        },
        "fada8250faf946d2a527de651a16f304": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe00ce45ecaf414d8945db61461d8846": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe17eaa65b6f45a7be958080d22c881d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_061091dab5c044e3a7f262c06356e720",
              "IPY_MODEL_aa2e011799c94c0e9ceb33f496337499",
              "IPY_MODEL_e50d12e74a3944c5a3276b427d8b9769"
            ],
            "layout": "IPY_MODEL_dc997a8f380a490c87b501a5600c1d0b"
          }
        },
        "fefc116b78804bf8a77c586cf9fce6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff0a780283644b0caab8f1040bef7a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff4cf821963b4eb9a872eeffb4fb60e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd72728d7c034146b3d6f4909f7a0778",
            "placeholder": "​",
            "style": "IPY_MODEL_556b19bec5224a868f332beb71f5cfdc",
            "value": " 2.20k/2.20k [00:00&lt;00:00, 262kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
